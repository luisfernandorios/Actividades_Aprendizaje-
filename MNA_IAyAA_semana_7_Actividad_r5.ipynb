{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luisfernandorios/Actividades_Aprendizaje-/blob/main/MNA_IAyAA_semana_7_Actividad_r5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Maestría en Inteligencia Artificial Aplicada**\n",
        "##**Curso: Inteligencia Artificial y Aprendizaje Automático**\n",
        "###Tecnológico de Monterrey\n",
        "###Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "## **Adtividad de la Semana 7**\n",
        "###**Red Neuronal Artificial - Perceptrón Multicapa : Multilayer Perceptrón (MLP)**\n"
      ],
      "metadata": {
        "id": "VFj0sSM06dYa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nombres y matrículas de los integrantes del equipo:**\n",
        "\n",
        "*   Alberto Jose Garcia Porras (A01793509)\n",
        "*   Carlos Julio León Caicedo (A01793947)\n",
        "*   Luis Fernando Ríos Piedra (A00453954)\n",
        "*   Marco Antonio Vázquez Morales (A01793704)"
      ],
      "metadata": {
        "id": "Qgrvy0RGB9XI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En cada sección deberás incluir todas las líneas de código necesarias para responder a cada uno de los ejercicios."
      ],
      "metadata": {
        "id": "FrJ2ahMODVj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Contexto del problema y el conjunto de datos a utilizar**\n",
        "\n",
        "El objetivo es determinar el impacto generado por un anuncio de una marca de cosméticos en Facebook, en el cual se intentaron varias variantes en la manera de mostrar el anuncio. Siguiendo el [artículo](https://www.semanticscholar.org/paper/Predicting-social-media-performance-metrics-and-of-Moro-Rita/dec55692590820754b53c916e29bb2b42c0e5104), deberás considerar como predictores o variables de entrada aquellas que se indican en la Tabla 3. Por otro lado, en la Tabla 2, los autores consideran varios casos para la variable de salida, intentando determinar cuál puede ser el mejor caso para medir el éxito de la campaña. Para este ejercicio deberás considerar únicamente los siguientes tres casos como variable de salida: “Lifetime post consumers”, “Lifetime People who have liked a Page and engaged with a post” y “Likes”. \n",
        "\n",
        "El conjunto de datos a utilizar es el de la siguiente liga de la UCI:\n",
        "https://archive.ics.uci.edu/ml/datasets/Facebook+metrics\n"
      ],
      "metadata": {
        "id": "sxlSJ-k7NYTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Incluye aquí todos módulos, librerías y paquetes que requieras.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests, zipfile\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import make_scorer,classification_report\n",
        "from imblearn.metrics import geometric_mean_score, classification_report_imbalanced\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold,RepeatedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import validation_curve\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from io import BytesIO\n",
        "\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from imblearn.under_sampling import EditedNearestNeighbours\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "\n",
        "from imblearn.combine import SMOTEENN\n",
        "from imblearn.combine import SMOTETomek\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import make_pipeline\n",
        "from imblearn.pipeline import make_pipeline as make_pipeline_with_sampler\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neural_network import MLPRegressor"
      ],
      "metadata": {
        "id": "exXsscs-Dh-2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X25brD-gQdZM"
      },
      "source": [
        "#**Ejercicio-1.** \n",
        "\n",
        "En esta tarea considera únicamente la siguiente variable de salida que se concluye que es una de las mejores en el artículo antes citado:  ‘Lifetime People who have liked a Page and engaged with a post'. Renombra dicha variable como “LPE”. Como variables de entrada selecciona las 7 variables que indican los autores en la Tabla 3 del artículo citado.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00368/Facebook_metrics.zip'\n",
        "base_datos = path.split('/')[-1]\n",
        "req_path = requests.get(path)\n",
        "\n",
        "archivo_uci= zipfile.ZipFile(BytesIO(req_path.content))\n",
        "archivo_uci.extractall('/content/sample_data/Facebook_metrics')\n",
        "df = pd.read_csv(\"/content/sample_data/Facebook_metrics/dataset_Facebook.csv\",sep=';')\n",
        "\n",
        "df.rename(columns={'Lifetime People who have liked your Page and engaged with your post':'LPE'},\n",
        "               inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "3nU2GuWYCy6C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "baf6924c-1b78-4d5d-c22f-2827e9578774"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Page total likes    Type  Category  Post Month  Post Weekday  Post Hour  \\\n",
              "0            139441   Photo         2          12             4          3   \n",
              "1            139441  Status         2          12             3         10   \n",
              "2            139441   Photo         3          12             3          3   \n",
              "3            139441   Photo         2          12             2         10   \n",
              "4            139441   Photo         2          12             2          3   \n",
              "\n",
              "   Paid  Lifetime Post Total Reach  Lifetime Post Total Impressions  \\\n",
              "0   0.0                       2752                             5091   \n",
              "1   0.0                      10460                            19057   \n",
              "2   0.0                       2413                             4373   \n",
              "3   1.0                      50128                            87991   \n",
              "4   0.0                       7244                            13594   \n",
              "\n",
              "   Lifetime Engaged Users  Lifetime Post Consumers  \\\n",
              "0                     178                      109   \n",
              "1                    1457                     1361   \n",
              "2                     177                      113   \n",
              "3                    2211                      790   \n",
              "4                     671                      410   \n",
              "\n",
              "   Lifetime Post Consumptions  \\\n",
              "0                         159   \n",
              "1                        1674   \n",
              "2                         154   \n",
              "3                        1119   \n",
              "4                         580   \n",
              "\n",
              "   Lifetime Post Impressions by people who have liked your Page  \\\n",
              "0                                               3078              \n",
              "1                                              11710              \n",
              "2                                               2812              \n",
              "3                                              61027              \n",
              "4                                               6228              \n",
              "\n",
              "   Lifetime Post reach by people who like your Page   LPE  comment    like  \\\n",
              "0                                              1640   119        4    79.0   \n",
              "1                                              6112  1108        5   130.0   \n",
              "2                                              1503   132        0    66.0   \n",
              "3                                             32048  1386       58  1572.0   \n",
              "4                                              3200   396       19   325.0   \n",
              "\n",
              "   share  Total Interactions  \n",
              "0   17.0                 100  \n",
              "1   29.0                 164  \n",
              "2   14.0                  80  \n",
              "3  147.0                1777  \n",
              "4   49.0                 393  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-484c5b5b-f6fc-49d1-80a6-3f91b4f2bc88\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Page total likes</th>\n",
              "      <th>Type</th>\n",
              "      <th>Category</th>\n",
              "      <th>Post Month</th>\n",
              "      <th>Post Weekday</th>\n",
              "      <th>Post Hour</th>\n",
              "      <th>Paid</th>\n",
              "      <th>Lifetime Post Total Reach</th>\n",
              "      <th>Lifetime Post Total Impressions</th>\n",
              "      <th>Lifetime Engaged Users</th>\n",
              "      <th>Lifetime Post Consumers</th>\n",
              "      <th>Lifetime Post Consumptions</th>\n",
              "      <th>Lifetime Post Impressions by people who have liked your Page</th>\n",
              "      <th>Lifetime Post reach by people who like your Page</th>\n",
              "      <th>LPE</th>\n",
              "      <th>comment</th>\n",
              "      <th>like</th>\n",
              "      <th>share</th>\n",
              "      <th>Total Interactions</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>139441</td>\n",
              "      <td>Photo</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2752</td>\n",
              "      <td>5091</td>\n",
              "      <td>178</td>\n",
              "      <td>109</td>\n",
              "      <td>159</td>\n",
              "      <td>3078</td>\n",
              "      <td>1640</td>\n",
              "      <td>119</td>\n",
              "      <td>4</td>\n",
              "      <td>79.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>139441</td>\n",
              "      <td>Status</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>10</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10460</td>\n",
              "      <td>19057</td>\n",
              "      <td>1457</td>\n",
              "      <td>1361</td>\n",
              "      <td>1674</td>\n",
              "      <td>11710</td>\n",
              "      <td>6112</td>\n",
              "      <td>1108</td>\n",
              "      <td>5</td>\n",
              "      <td>130.0</td>\n",
              "      <td>29.0</td>\n",
              "      <td>164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>139441</td>\n",
              "      <td>Photo</td>\n",
              "      <td>3</td>\n",
              "      <td>12</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2413</td>\n",
              "      <td>4373</td>\n",
              "      <td>177</td>\n",
              "      <td>113</td>\n",
              "      <td>154</td>\n",
              "      <td>2812</td>\n",
              "      <td>1503</td>\n",
              "      <td>132</td>\n",
              "      <td>0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>139441</td>\n",
              "      <td>Photo</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>1.0</td>\n",
              "      <td>50128</td>\n",
              "      <td>87991</td>\n",
              "      <td>2211</td>\n",
              "      <td>790</td>\n",
              "      <td>1119</td>\n",
              "      <td>61027</td>\n",
              "      <td>32048</td>\n",
              "      <td>1386</td>\n",
              "      <td>58</td>\n",
              "      <td>1572.0</td>\n",
              "      <td>147.0</td>\n",
              "      <td>1777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>139441</td>\n",
              "      <td>Photo</td>\n",
              "      <td>2</td>\n",
              "      <td>12</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7244</td>\n",
              "      <td>13594</td>\n",
              "      <td>671</td>\n",
              "      <td>410</td>\n",
              "      <td>580</td>\n",
              "      <td>6228</td>\n",
              "      <td>3200</td>\n",
              "      <td>396</td>\n",
              "      <td>19</td>\n",
              "      <td>325.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>393</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-484c5b5b-f6fc-49d1-80a6-3f91b4f2bc88')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-484c5b5b-f6fc-49d1-80a6-3f91b4f2bc88 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-484c5b5b-f6fc-49d1-80a6-3f91b4f2bc88');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "In2zOQWwb7fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Llenado de dato único dato nulo encontrado\n",
        "df['Paid']=df['Paid'].fillna(0)"
      ],
      "metadata": {
        "id": "YoZ9w8BQbzJP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Separación de variables numéricas y categóricas del df original\n",
        "#Creación de dummies para variable Type\n",
        "df_nums = df.select_dtypes(exclude='object')\n",
        "df_objs = df.select_dtypes(include='object')\n",
        "df_objs = pd.get_dummies(df_objs,drop_first=False)\n",
        "final_df = pd.concat([df_nums,df_objs],axis=1)\n",
        "final_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5K1WQxbtX2oI",
        "outputId": "7cac0023-d339-4b63-f3f8-4ba5a98d398b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Page total likes', 'Category', 'Post Month', 'Post Weekday',\n",
              "       'Post Hour', 'Paid', 'Lifetime Post Total Reach',\n",
              "       'Lifetime Post Total Impressions', 'Lifetime Engaged Users',\n",
              "       'Lifetime Post Consumers', 'Lifetime Post Consumptions',\n",
              "       'Lifetime Post Impressions by people who have liked your Page',\n",
              "       'Lifetime Post reach by people who like your Page', 'LPE', 'comment',\n",
              "       'like', 'share', 'Total Interactions', 'Type_Link', 'Type_Photo',\n",
              "       'Type_Status', 'Type_Video'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generar df con variables finales para el modelo\n",
        "var_input=final_df[['Category', 'Page total likes', 'Type_Photo', 'Type_Status',\n",
        "       'Type_Video', 'Post Month', 'Post Hour', 'Post Weekday', 'Paid','LPE']]\n",
        "# print(df[['Category', 'Page total likes', 'Type', 'Post Month', 'Post Hour', 'Post Weekday', 'Paid']])\n"
      ],
      "metadata": {
        "id": "0ulkqXVGCy97"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(var_input['LPE'])"
      ],
      "metadata": {
        "id": "Fb553D6OUt2y",
        "outputId": "9996bd0d-696f-4cdf-860d-2dd8bd704f1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6537738210>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q75,q25=np.percentile(var_input['LPE'],[75,25])\n",
        "IQR=q75-q25\n",
        "upper_limit=q75+1.5*(IQR)\n",
        "upper_limit"
      ],
      "metadata": {
        "id": "lmT98anyjvuC",
        "outputId": "7ebb564e-3641-4473-f20e-0c2a56d8b464",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1204.125"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ser=pd.Series(var_input['LPE'])"
      ],
      "metadata": {
        "id": "gWtNVlM6lGsX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ser[ser<1204.125])"
      ],
      "metadata": {
        "id": "NM4uCLcDlWM3",
        "outputId": "39d89da1-6b7e-4a45-a177-9e6c2c753530",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "441"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(var_input[\"LPE\"])"
      ],
      "metadata": {
        "id": "hy-O8m47mTPN",
        "outputId": "1a81ca4d-d1a9-41ad-d01f-82cd9824012e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6536ff3b10>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import power_transform\n",
        "# Transf4 = power_transform(np.array(ser).reshape(-1,1),method=\"box-cox\")\n",
        "Transf4=np.log(ser)"
      ],
      "metadata": {
        "id": "mfF4EA0vn-p1"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yy=pd.Series(np.ravel(Transf4))"
      ],
      "metadata": {
        "id": "CoDOCmhsp60E"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.distplot(Transf4)"
      ],
      "metadata": {
        "id": "ZPJBKli8pBwm",
        "outputId": "b9f33292-2886-42cb-dcdb-ff986ebfa867",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        }
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f6557799b10>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var_input.info()"
      ],
      "metadata": {
        "id": "vAV3Q59BUZuM",
        "outputId": "9e0f0dea-a49f-4cdd-a2a3-cc4b2eb94740",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 500 entries, 0 to 499\n",
            "Data columns (total 10 columns):\n",
            " #   Column            Non-Null Count  Dtype  \n",
            "---  ------            --------------  -----  \n",
            " 0   Category          500 non-null    int64  \n",
            " 1   Page total likes  500 non-null    int64  \n",
            " 2   Type_Photo        500 non-null    uint8  \n",
            " 3   Type_Status       500 non-null    uint8  \n",
            " 4   Type_Video        500 non-null    uint8  \n",
            " 5   Post Month        500 non-null    int64  \n",
            " 6   Post Hour         500 non-null    int64  \n",
            " 7   Post Weekday      500 non-null    int64  \n",
            " 8   Paid              500 non-null    float64\n",
            " 9   LPE               500 non-null    int64  \n",
            "dtypes: float64(1), int64(6), uint8(3)\n",
            "memory usage: 28.9 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var_input.describe(include='all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "aIeXsS5eOwfP",
        "outputId": "2d0ac862-ce0d-4c1e-df63-30e480225ea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Category  Page total likes  Type_Photo  Type_Status  Type_Video  \\\n",
              "count  500.000000        500.000000  500.000000   500.000000  500.000000   \n",
              "mean     1.880000     123194.176000    0.852000     0.090000    0.014000   \n",
              "std      0.852675      16272.813214    0.355456     0.286468    0.117608   \n",
              "min      1.000000      81370.000000    0.000000     0.000000    0.000000   \n",
              "25%      1.000000     112676.000000    1.000000     0.000000    0.000000   \n",
              "50%      2.000000     129600.000000    1.000000     0.000000    0.000000   \n",
              "75%      3.000000     136393.000000    1.000000     0.000000    0.000000   \n",
              "max      3.000000     139441.000000    1.000000     1.000000    1.000000   \n",
              "\n",
              "       Post Month   Post Hour  Post Weekday        Paid          LPE  \n",
              "count  500.000000  500.000000    500.000000  500.000000   500.000000  \n",
              "mean     7.038000    7.840000      4.150000    0.278000   609.986000  \n",
              "std      3.307936    4.368589      2.030701    0.448462   612.725618  \n",
              "min      1.000000    1.000000      1.000000    0.000000     9.000000  \n",
              "25%      4.000000    3.000000      2.000000    0.000000   291.000000  \n",
              "50%      7.000000    9.000000      4.000000    0.000000   412.000000  \n",
              "75%     10.000000   11.000000      6.000000    1.000000   656.250000  \n",
              "max     12.000000   23.000000      7.000000    1.000000  4376.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b387ceb-405b-4302-83a8-7f56569908fa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Page total likes</th>\n",
              "      <th>Type_Photo</th>\n",
              "      <th>Type_Status</th>\n",
              "      <th>Type_Video</th>\n",
              "      <th>Post Month</th>\n",
              "      <th>Post Hour</th>\n",
              "      <th>Post Weekday</th>\n",
              "      <th>Paid</th>\n",
              "      <th>LPE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "      <td>500.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.880000</td>\n",
              "      <td>123194.176000</td>\n",
              "      <td>0.852000</td>\n",
              "      <td>0.090000</td>\n",
              "      <td>0.014000</td>\n",
              "      <td>7.038000</td>\n",
              "      <td>7.840000</td>\n",
              "      <td>4.150000</td>\n",
              "      <td>0.278000</td>\n",
              "      <td>609.986000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.852675</td>\n",
              "      <td>16272.813214</td>\n",
              "      <td>0.355456</td>\n",
              "      <td>0.286468</td>\n",
              "      <td>0.117608</td>\n",
              "      <td>3.307936</td>\n",
              "      <td>4.368589</td>\n",
              "      <td>2.030701</td>\n",
              "      <td>0.448462</td>\n",
              "      <td>612.725618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>81370.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>9.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>112676.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>291.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>129600.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>412.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>136393.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>656.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>139441.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4376.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b387ceb-405b-4302-83a8-7f56569908fa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1b387ceb-405b-4302-83a8-7f56569908fa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1b387ceb-405b-4302-83a8-7f56569908fa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-tEBNNeBwlO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AwropDGy9QK2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos los atributos de entrada como 'X' y a la variable de salida como 'Y'\n",
        "# Para este ejercicio deberás considerar únicamente los siguientes tres casos como variable de salida: \n",
        "# “Lifetime post consumers”, “Lifetime People who have liked a Page and engaged with a post” y “Likes”.\n",
        "\n",
        "X = var_input.drop('LPE',axis=1)\n",
        "y = var_input['LPE']\n",
        "\n",
        "#Variables de salida de la Tabla 2, los autores consideran varios casos para la variable de salida\n",
        "# Y=df['Lifetime post total reach','Lifetime post total impressions','Lifetime engaged users','Lifetime post consumers',\n",
        "#      'Lifetime post consumotions','Lifetime post impressions by people who have liked a page','Lifetime post reach by people who like a page'\n",
        "#      'Lifetime people who have liked a page and engaged with a post','Comments','Likes','Shares','Total interactions']"
      ],
      "metadata": {
        "id": "gOJhhc8dZ1UP"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "var_input.columns\n"
      ],
      "metadata": {
        "id": "C1Rw4YhD4Ddr",
        "outputId": "888923c1-41b4-4321-9259-c8ad50db5b26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Category', 'Page total likes', 'Type_Photo', 'Type_Status',\n",
              "       'Type_Video', 'Post Month', 'Post Hour', 'Post Weekday', 'Paid', 'LPE'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "e3k8QL-SAhI1",
        "outputId": "524ddf33-9861-457f-9b59-9bf27ff55029",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Page total likes  Category  Type_Photo  Type_Status  Type_Video  \\\n",
              "0            2.000000       1.0         0.0          0.0        12.0   \n",
              "1            2.000000       0.0         1.0          0.0        12.0   \n",
              "2            2.000000       1.0         0.0          0.0        12.0   \n",
              "3            2.000000       1.0         0.0          0.0        12.0   \n",
              "4            2.000000       1.0         0.0          0.0        12.0   \n",
              "..                ...       ...         ...          ...         ...   \n",
              "495          1.064111       1.0         0.0          0.0         1.0   \n",
              "496          1.000000       1.0         0.0          0.0         1.0   \n",
              "497          1.000000       1.0         0.0          0.0         1.0   \n",
              "498          1.000000       1.0         0.0          0.0         1.0   \n",
              "499          1.000000       1.0         0.0          0.0         1.0   \n",
              "\n",
              "     Post Month  Post Hour  Post Weekday  Paid  Type_Photo_2  Type_Status_2  \\\n",
              "0           3.0        4.0           0.0   1.0           0.0            0.0   \n",
              "1          10.0        3.0           0.0   0.0           1.0            0.0   \n",
              "2           3.0        3.0           0.0   1.0           0.0            0.0   \n",
              "3          10.0        2.0           1.0   1.0           0.0            0.0   \n",
              "4           3.0        2.0           0.0   1.0           0.0            0.0   \n",
              "..          ...        ...           ...   ...           ...            ...   \n",
              "495         2.0        7.0           0.0   1.0           0.0            0.0   \n",
              "496         8.0        5.0           0.0   1.0           0.0            0.0   \n",
              "497         2.0        5.0           0.0   1.0           0.0            0.0   \n",
              "498        11.0        4.0           0.0   1.0           0.0            0.0   \n",
              "499         4.0        4.0           0.0   1.0           0.0            0.0   \n",
              "\n",
              "     Type_Video_2  Paid_2  \n",
              "0             0.0   119.0  \n",
              "1             0.0  1108.0  \n",
              "2             0.0   132.0  \n",
              "3             1.0  1386.0  \n",
              "4             0.0   396.0  \n",
              "..            ...     ...  \n",
              "495           0.0   392.0  \n",
              "496           0.0   301.0  \n",
              "497           0.0   363.0  \n",
              "498           0.0   370.0  \n",
              "499           0.0   316.0  \n",
              "\n",
              "[500 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-036b2a42-79c0-4e3c-8f95-1f98482c2647\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Page total likes</th>\n",
              "      <th>Category</th>\n",
              "      <th>Type_Photo</th>\n",
              "      <th>Type_Status</th>\n",
              "      <th>Type_Video</th>\n",
              "      <th>Post Month</th>\n",
              "      <th>Post Hour</th>\n",
              "      <th>Post Weekday</th>\n",
              "      <th>Paid</th>\n",
              "      <th>Type_Photo_2</th>\n",
              "      <th>Type_Status_2</th>\n",
              "      <th>Type_Video_2</th>\n",
              "      <th>Paid_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>119.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1108.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>132.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1386.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>396.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>1.064111</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>392.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>301.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>363.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>370.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>316.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-036b2a42-79c0-4e3c-8f95-1f98482c2647')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-036b2a42-79c0-4e3c-8f95-1f98482c2647 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-036b2a42-79c0-4e3c-8f95-1f98482c2647');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "cVFQdyRSAkGZ",
        "outputId": "72d9d657-5064-4c6d-db4a-b218b3ba190a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      1.025189\n",
              "1      1.251660\n",
              "2      1.028166\n",
              "3      1.315319\n",
              "4      1.088619\n",
              "         ...   \n",
              "495    1.087703\n",
              "496    1.066865\n",
              "497    1.081063\n",
              "498    1.082665\n",
              "499    1.070300\n",
              "Name: LPE, Length: 500, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-2.**\n",
        "\n",
        "Realiza una partición de los datos con 100 datos de Prueba y el resto para entrenamiento y validación."
      ],
      "metadata": {
        "id": "xZhr2hkECzVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Definimos los atributos de entrada como 'X' y a la variable de salida como 'Y'\n",
        "# # Para este ejercicio deberás considerar únicamente los siguientes tres casos como variable de salida: \n",
        "# # “Lifetime post consumers”, “Lifetime People who have liked a Page and engaged with a post” y “Likes”.\n",
        "\n",
        "# var_otput = ['LPE']\n",
        "# X = df[var_input]\n",
        "# Y = df[var_otput]\n",
        "\n",
        "# #Variables de salida de la Tabla 2, los autores consideran varios casos para la variable de salida\n",
        "# # Y=df['Lifetime post total reach','Lifetime post total impressions','Lifetime engaged users','Lifetime post consumers',\n",
        "# #      'Lifetime post consumotions','Lifetime post impressions by people who have liked a page','Lifetime post reach by people who like a page'\n",
        "# #      'Lifetime people who have liked a page and engaged with a post','Comments','Likes','Shares','Total interactions']"
      ],
      "metadata": {
        "id": "kGfAoOPkC1PP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Partición de datos, test_size=100*100/500= 20%\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, yy, test_size=.20)\n"
      ],
      "metadata": {
        "id": "GOydw5OGC1MJ"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_testt=X_test.reset_index().drop('index', axis='columns') \n",
        "y_testt=y_test.reset_index().drop('index', axis='columns') "
      ],
      "metadata": {
        "id": "n45H1kF3SFtX"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.ravel(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSACJ_P3i6bU",
        "outputId": "da00f595-fb0d-4c42-daa9-aebd861e1d97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2126,  168,  762,  306,  167, 1834,  157,  403,  230,  391,  297,\n",
              "        347, 1292,  293,  333, 1275, 1613,  814,  140,  340, 2119,  508,\n",
              "       1200,  621,  705,  732, 1578,  477,  269,  630,  305,  559,  316,\n",
              "        123,  152, 3430,  436, 1513,  342,  278,  435,  475, 2278,  985,\n",
              "         59,  998, 1542,  119,  437,  646,  395, 1101,  340, 2256,  283,\n",
              "        220,  393,  327,  319,   32,  240,  699,  413,  220,  471,  379,\n",
              "        305,    9,  301,  576,  441,  275, 1779, 1276,  422,  287, 1146,\n",
              "        240,  348,  456,   93,  375,  327,  392,  414,  392,  557,  100,\n",
              "        289, 1356,  348,  300, 1086,  774,  563,   58, 1564,  465,  335,\n",
              "        482,  236, 3798,  884,  885,  774, 1331,   92,  606,  156,  399,\n",
              "        476,  248, 2342,  475,  447,  357,  327,  697,  594,  106, 1052,\n",
              "        126,  363,  211,  590,  134,  462,  185, 1661,  403,  514,  447,\n",
              "        268,  319,  569,  760,  291, 1233,  363,  236,  740,  375, 2252,\n",
              "        907, 3316,  131,  408,  924, 3300,  347, 1353,  516, 1609,  408,\n",
              "         15,  740,  196,  642,  181, 1392,  351,  584,  460,  454,  323,\n",
              "       1292,  323,  176, 1905,  101,  309,  397,  604,  246,  445,   15,\n",
              "        470,  660,  788,  367,  981, 4318,  630,  530,  363,  537,  361,\n",
              "        684,  123,  398,  239,  493,  382,  453,  244,  387,  263,  621,\n",
              "        463,  351,   93,  428,  404,  277,  440,  132,  461,  183,  656,\n",
              "        432,  280,  351,  143,  408, 1035,  180,  459,  570,  390,  361,\n",
              "        489, 2806,  593,   77, 1185,  315, 1349,  393,   33,  398,  401,\n",
              "        287,  650, 1975,  165,  497,  459,  340,  471,  505,  389,  266,\n",
              "        538,  438,  537,  307,  724,  796,  191, 1395,  365,  298, 1354,\n",
              "        501,  377,  360,  166,  484, 1978,  175,  253,  389,  977,  428,\n",
              "        346,  379,  316,  429,  355,  328,  251,  306, 1075, 1604,  436,\n",
              "        187,  488,  166,  259,  598,  995,  169, 1756,  154,  143,  271,\n",
              "        248, 1225, 2099,  314, 1250,  559,  385,   17,  361,  348,  467,\n",
              "        309,  920, 2602, 1307,  460, 1162,  483,  237,  729,  583,  431,\n",
              "        448,  137, 2361,  469,  708,  504,  801, 1008,  355,  262, 1790,\n",
              "        403,  175,  252,  439,  497,  194, 4376,  375,  222,  280,  288,\n",
              "        865,  305,  341,  194,  232,  506, 1020,  570,  340,  583,  487,\n",
              "       1716,  445,  316, 3014,  546,  280, 4104,  876,  742,  268, 1034,\n",
              "        555,  384,  345,  662,  349,  403,  446,  432,  706,  289,  265,\n",
              "        233,  497,  341,  505,  563,  221,  441,  636,   99,  843,  200,\n",
              "        328,  628,  701,  389,  757,  722,  440,  370,  472,  975,  583,\n",
              "        199,  422,  492,  322,  342,  279,  420,  705, 1936,  469,  704,\n",
              "       1016,  392,  411,  403])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-3.**\n",
        "Definirás tus propias funciones de errores para este problema de regresión. Los errores que utilizarás son la raíz cuadrada del error cuadrático medio RMSE, el error absoluto medio MAE y el error porcentual absoluto medio MAPE."
      ],
      "metadata": {
        "id": "NCunuooTC2W3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Definiendo función RMSE\n",
        "def mi_RMSE(y_val,yhatVal):\n",
        "  return np.mean(np.sqrt(np.abs(y_val-yhatVal)))\n",
        "#Definiendo función MAE\n",
        "def mi_MAE(y_val,yhatVal):\n",
        "  return np.mean(np.abs(y_val-yhatVal))\n",
        "#Definiendo función MAPE\n",
        "def mi_MAPE(y_val,yhatVal):\n",
        "  return np.mean(np.abs((y_val-yhatVal)/y_val))*100"
      ],
      "metadata": {
        "id": "YXlcSWA-C4Dj"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-4.**\n",
        "En la página de la UCI, así como en el artículo de los autores previamente citado encuentras información en relación al significado de cada variable. Haz una análisis de tus datos y lleva a cabo las transformaciones que consideres adecuadas tanto en los datos de entrada, como en las de salida.\n",
        "Utiliza un Pipeline para evitar el filtrado de información.\n"
      ],
      "metadata": {
        "id": "chqk9jIDC5Pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformaciones a factores numéricos de entrada:\n",
        "num_pipe = Pipeline(steps = [('impMediana', SimpleImputer(strategy='median')),\n",
        "                                 ('escalaNum', MinMaxScaler(feature_range=(1,2)))])   \n",
        "num_pipe_nombres = [ 'Page total likes', 'LPE']\n",
        "\n",
        "# Transformaciones a factores categóricos de entrada:\n",
        "catImp_pipe = Pipeline(steps = [('impModa', SimpleImputer(strategy='most_frequent'))])  \n",
        "catImp_pipe_nombres = ['Category', 'Type_Photo', 'Type_Status',\n",
        "       'Type_Video', 'Post Month', 'Post Hour', 'Post Weekday','Paid']\n",
        "\n",
        "catOHE_pipe = Pipeline(steps = [('OneHotE', OneHotEncoder(drop='first',handle_unknown='ignore'))])\n",
        "catOHE_pipe_nombres = ['Type_Photo', 'Type_Status',\n",
        "       'Type_Video','Paid']\n",
        "\n",
        "# Conjuntamos las transformaciones numéricas y categóricas que se estarán aplicando a los datos de entrada:\n",
        "columnasTransformer = ColumnTransformer(transformers = [('numpipe', num_pipe, num_pipe_nombres),\n",
        "                                                        ('catimp', catImp_pipe, catImp_pipe_nombres),\n",
        "                                                        ('catohe', catOHE_pipe, catOHE_pipe_nombres)],\n",
        "                                        remainder='passthrough')\n",
        "\n",
        "pipe = Pipeline(steps=[('ct',columnasTransformer)])"
      ],
      "metadata": {
        "id": "RBVSFwK4C6g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformaciones a factores numéricos de entrada:\n",
        "num_pipe = Pipeline(steps = [('impMediana', SimpleImputer(strategy='median')),\n",
        "                                 ('escalaNum', MinMaxScaler(feature_range=(1,2)))])   \n",
        "num_pipe_nombres = [ 'Page total likes']\n",
        "\n",
        "# Transformaciones a factores categóricos de entrada:\n",
        "catImp_pipe = Pipeline(steps = [('impModa', SimpleImputer(strategy='most_frequent'))])  \n",
        "catImp_pipe_nombres = ['Category', 'Type_Photo', 'Type_Status',\n",
        "       'Type_Video', 'Post Month', 'Post Hour', 'Post Weekday','Paid']\n",
        "\n",
        "catOHE_pipe = Pipeline(steps = [('OneHotE', OneHotEncoder(drop='first',handle_unknown='ignore'))])\n",
        "catOHE_pipe_nombres = ['Type_Photo', 'Type_Status',\n",
        "       'Type_Video','Paid']\n",
        "\n",
        "# Conjuntamos las transformaciones numéricas y categóricas que se estarán aplicando a los datos de entrada:\n",
        "columnasTransformer = ColumnTransformer(transformers = [('numpipe', num_pipe, num_pipe_nombres),\n",
        "                                                        ('catimp', catImp_pipe, catImp_pipe_nombres),\n",
        "                                                        ('catohe', catOHE_pipe, catOHE_pipe_nombres)],\n",
        "                                        remainder='passthrough')\n",
        "\n",
        "pipe = Pipeline(steps=[('ct',columnasTransformer)])"
      ],
      "metadata": {
        "id": "C-08lIAiBDlr"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe"
      ],
      "metadata": {
        "id": "mRYUSa1IuNfe",
        "outputId": "d6944bc6-edf7-4c0f-bf9f-d9b97d85e423",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('ct',\n",
              "                 ColumnTransformer(remainder='passthrough',\n",
              "                                   transformers=[('numpipe',\n",
              "                                                  Pipeline(steps=[('impMediana',\n",
              "                                                                   SimpleImputer(strategy='median')),\n",
              "                                                                  ('escalaNum',\n",
              "                                                                   MinMaxScaler(feature_range=(1,\n",
              "                                                                                               2)))]),\n",
              "                                                  ['Page total likes']),\n",
              "                                                 ('catimp',\n",
              "                                                  Pipeline(steps=[('impModa',\n",
              "                                                                   SimpleImputer(strategy='most_frequent'))]),\n",
              "                                                  ['Category', 'Type_Photo',\n",
              "                                                   'Type_Status', 'Type_Video',\n",
              "                                                   'Post Month', 'Post Hour',\n",
              "                                                   'Post Weekday', 'Paid']),\n",
              "                                                 ('catohe',\n",
              "                                                  Pipeline(steps=[('OneHotE',\n",
              "                                                                   OneHotEncoder(drop='first',\n",
              "                                                                                 handle_unknown='ignore'))]),\n",
              "                                                  ['Type_Photo', 'Type_Status',\n",
              "                                                   'Type_Video', 'Paid'])]))])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.fit(var_input)\n",
        "\n",
        "var_input=pipe.transform(var_input)\n",
        "\n",
        "var_input_df = pd.DataFrame(var_input)\n"
      ],
      "metadata": {
        "id": "JOxF8Q-10yW0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "var_input_df.columns=['Page total likes', 'LPE', 'Category','Type_Photo', 'Type_Status','Type_Video', 'Post Month','Post Hour', 'Post Weekday','Paid', 'Type_Photo_2', 'Type_Status_2', 'Type_Video_2', 'Paid_2']"
      ],
      "metadata": {
        "id": "rm2g2U3k4kmh"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "var_input=var_input_df"
      ],
      "metadata": {
        "id": "e2itivs946sP"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# var_input_df.columns=(['Category', 'Page total likes', 'Type_Photo', 'Type_Status',\n",
        "#        'Type_Video', 'Post Month', 'Post Hour', 'Post Weekday', 'Paid', 'LPE'],\n",
        "#       dtype='object')"
      ],
      "metadata": {
        "id": "e-rlrSYZ5OVL",
        "outputId": "359976ea-2be5-4000-f5da-d6443b5d6ef3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-f078ed124136>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    dtype='object')\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-5.**\n",
        "Utiliza la función Dummy para modelos de regresión de scikit-learn con el conjunto que tienes de datos de entrenamiento y validación. Para ello particiónalos en 100 para validación y 300 para entrenamiento. Encuentra los errores RMSE, MAE y MAPE para los conjuntos de entrenamiento y validación. \n",
        "\n",
        "Estos serán tus errores máximos que deberás tomar como referencia en el resto de la actividad. Consulta su documentación correspondiente: https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html"
      ],
      "metadata": {
        "id": "Rv7KFq-mC7PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Partición de datos, test_size=100*100/400= 25%\n",
        "Xtrain, X_val, ytrain, y_val = train_test_split(X_train, y_train, test_size=.25)"
      ],
      "metadata": {
        "id": "jaDj3kawC9B6"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.common import random_state\n",
        "from sklearn.dummy import DummyRegressor\n",
        "\n",
        "modeloDummy = DummyRegressor(strategy=\"mean\")\n",
        "\n",
        "XtrainFit_d = columnasTransformer.fit(Xtrain)   \n",
        "XtrainTransf_d = XtrainFit_d.transform(Xtrain) \n",
        "\n",
        "modeloDummy=modeloDummy.fit(XtrainTransf_d, ytrain)\n",
        "\n",
        "XvalTransf_d = XtrainFit_d.transform(X_val)\n",
        "yhat = modeloDummy.predict(XvalTransf_d)\n",
        "\n",
        "#modeloDummy.score(X_train,y_train)\n",
        "\n",
        "print('Valor de RMSE a superar: %.4f' % np.mean(mi_RMSE(y_val,yhat)))\n",
        "print(\"Valor del MAE a superar: %.4f\" % np.mean(mi_MAE(y_val,yhat)))\n",
        "print(\"Valor del MAPE a superar: %.4f\" % np.mean(mi_MAPE(y_val,yhat)))"
      ],
      "metadata": {
        "id": "4tQxQROVC9Us",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "829a1219-355b-4fac-c270-e2bdd6797a28"
      },
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valor de RMSE a superar: 0.6932\n",
            "Valor del MAE a superar: 0.6138\n",
            "Valor del MAPE a superar: 10.7640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-6.**\n",
        "Usando los modelos de regresión lineal múltiple, el bosque aleatorio y el perceptrón multicapa con sus valores predeterminados, lleva a cabo su entrenamiento con repeticiones de validación cruzada (RepeatedKFold) y desplegando los errores RMSE, MAE y MAPE. Recuerda evitar el filtrado de\n",
        "información usando los datos que obtuviste en el ejercicio 2. Incluye las conclusiones sobre el mejor modelo encontrado en esta primera aproximación. En particular ¿hay alguno sobreentrenado o subentrenado? NOTA: Recuerda que puedes aumentar en dado caso el número máximo de iteraciones para que todos los modelos converjan."
      ],
      "metadata": {
        "id": "W2S7LI0NC9wE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cross_validate"
      ],
      "metadata": {
        "id": "7JP0JXPJKGtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_models():\n",
        "  modelos = list()\n",
        "  nombres = list()\n",
        "\n",
        "  # LR - Regresión Lineal Multiiple:\n",
        "  modelos.append(LinearRegression())\n",
        "  nombres.append('LLM')\n",
        "\n",
        "  # DT - Bosque Aleatorio:\n",
        "  modelos.append(RandomForestClassifier())\n",
        "  nombres.append('RF')\n",
        "  \n",
        "  # MLP - Red Neuronal Artificial / Perceptrón Lineal Multicapa:  \n",
        "  # modelos.append(MLPClassifier( max_iter=3000))\n",
        "  # nombres.append('MLP')\n",
        "  #modelos.append(MLPRegressor(max_iter=3000))\n",
        "  #nombres.append('MLP')\n",
        "  \n",
        "  return modelos, nombres"
      ],
      "metadata": {
        "id": "x6uBleJUC_AU"
      },
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "⚠ **¿Es el MLPClassifier o el MLPRegressor?**\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "TjZBr0_aXd9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "id": "ss5zgvycwHZL",
        "outputId": "79c20442-d3d5-45f2-b09b-c1c4c6ccf203",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Page total likes  Category  Type_Photo  Type_Status  Type_Video  \\\n",
              "85           1.961013       1.0         0.0          0.0        11.0   \n",
              "164          1.930895       1.0         0.0          0.0         9.0   \n",
              "229          1.865492       1.0         0.0          0.0         7.0   \n",
              "305          1.750288       1.0         0.0          0.0         6.0   \n",
              "32           1.990598       1.0         0.0          0.0        12.0   \n",
              "..                ...       ...         ...          ...         ...   \n",
              "373          1.545160       1.0         0.0          0.0         4.0   \n",
              "411          1.440116       1.0         0.0          0.0         4.0   \n",
              "481          1.088185       0.0         0.0          0.0         1.0   \n",
              "472          1.169672       1.0         0.0          0.0         2.0   \n",
              "283          1.770970       1.0         0.0          0.0         6.0   \n",
              "\n",
              "     Post Month  Post Hour  Post Weekday  Paid  Type_Photo_2  Type_Status_2  \\\n",
              "85          9.0        4.0           1.0   1.0           0.0            0.0   \n",
              "164        10.0        5.0           0.0   1.0           0.0            0.0   \n",
              "229        13.0        3.0           0.0   1.0           0.0            0.0   \n",
              "305         2.0        4.0           0.0   1.0           0.0            0.0   \n",
              "32          3.0        3.0           0.0   1.0           0.0            0.0   \n",
              "..          ...        ...           ...   ...           ...            ...   \n",
              "373         5.0        5.0           0.0   1.0           0.0            0.0   \n",
              "411        13.0        4.0           0.0   1.0           0.0            0.0   \n",
              "481         4.0        4.0           1.0   0.0           0.0            0.0   \n",
              "472        13.0        2.0           0.0   1.0           0.0            0.0   \n",
              "283         4.0        7.0           1.0   1.0           0.0            0.0   \n",
              "\n",
              "     Type_Video_2  Paid_2  \n",
              "85            1.0   266.0  \n",
              "164           0.0   204.0  \n",
              "229           0.0   348.0  \n",
              "305           0.0   569.0  \n",
              "32            0.0   268.0  \n",
              "..            ...     ...  \n",
              "373           0.0   758.0  \n",
              "411           0.0   335.0  \n",
              "481           1.0    59.0  \n",
              "472           0.0   466.0  \n",
              "283           1.0  1035.0  \n",
              "\n",
              "[100 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bad009dd-0b3a-4c22-84ae-209fb58f8f7c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Page total likes</th>\n",
              "      <th>Category</th>\n",
              "      <th>Type_Photo</th>\n",
              "      <th>Type_Status</th>\n",
              "      <th>Type_Video</th>\n",
              "      <th>Post Month</th>\n",
              "      <th>Post Hour</th>\n",
              "      <th>Post Weekday</th>\n",
              "      <th>Paid</th>\n",
              "      <th>Type_Photo_2</th>\n",
              "      <th>Type_Status_2</th>\n",
              "      <th>Type_Video_2</th>\n",
              "      <th>Paid_2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>1.961013</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>266.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>1.930895</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>204.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>1.865492</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>348.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305</th>\n",
              "      <td>1.750288</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>569.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1.990598</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>268.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>373</th>\n",
              "      <td>1.545160</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>758.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411</th>\n",
              "      <td>1.440116</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>335.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481</th>\n",
              "      <td>1.088185</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>59.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>472</th>\n",
              "      <td>1.169672</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>466.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>1.770970</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1035.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bad009dd-0b3a-4c22-84ae-209fb58f8f7c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bad009dd-0b3a-4c22-84ae-209fb58f8f7c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bad009dd-0b3a-4c22-84ae-209fb58f8f7c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modelos, nombres = get_models()\n",
        "\n",
        "scores_RMSEVal = list()\n",
        "scores_MAEVal = list()\n",
        "scores_MAPEVal = list()\n",
        "\n",
        "filas = 15\n",
        "columnas = 2\n",
        "\n",
        "A=np.zeros([filas, columnas])\n",
        "\n",
        "\n",
        "kf = RepeatedKFold(n_splits=5, n_repeats=3) \n",
        "\n",
        "  \n",
        "for i in range(len(modelos)):\n",
        "  j=0\n",
        "  if i ==0:\n",
        "    pipeline=Pipeline(steps=[('ct',columnasTransformer),('m',modelos[i])])\n",
        "  \n",
        "    for train_index, test_index in kf.split(X_test):\n",
        "      \n",
        "      \n",
        "\n",
        "      X__train, X__val = X_testt.loc[train_index], X_testt.loc[test_index]\n",
        "      y__train, y__val = y_testt.loc[train_index], y_testt.loc[test_index]\n",
        "      \n",
        "      pipeline.fit(X__train,np.ravel(y__train))\n",
        "      yhatVal=pipeline.predict(X__val)\n",
        "\n",
        "      scores_RMSEVal.append(mi_RMSE(np.ravel(y__val), yhatVal))\n",
        "      scores_MAEVal.append(mi_MAE(np.ravel(y__val), yhatVal))\n",
        "      scores_MAPEVal.append(mi_MAPE(np.ravel(y__val), yhatVal))\n",
        "      res=mi_MAPE(np.ravel(y__val), yhatVal)\n",
        "      \n",
        "      A[j,i]=res\n",
        "      j=j+1\n",
        "  # Desplegar información:\n",
        "    \n",
        "    print('>> %s:\\nRMSE: %.3f \\nMAE: %.3f  \\nMAPE: %.3f' % (nombres[i],\n",
        "                                                                 np.mean(scores_RMSEVal),\n",
        "                                                                 np.mean(scores_MAEVal),\n",
        "                                                                 np.mean(scores_MAPEVal)))\n",
        "\n",
        "  if i==1:\n",
        "\n",
        "    pipeline=Pipeline(steps=[('ct',columnasTransformer),('m',modelos[i])])\n",
        "\n",
        "    for train_index, test_index in kf.split(X_test):\n",
        "\n",
        "        X__train, X__val = X_testt.loc[train_index], X_testt.loc[test_index]\n",
        "        y__train, y__val = y_testt.loc[train_index], y_testt.loc[test_index]\n",
        "\n",
        "        y__train[[0]]=y__train[[0]].astype('int')\n",
        "        y__val[[0]]=y__val[[0]].astype('int')\n",
        " \n",
        "        pipeline.fit(X__train,np.ravel(y__train))\n",
        "        yhatVal=pipeline.predict(X__val)\n",
        "\n",
        "        scores_RMSEVal.append(mi_RMSE(np.ravel(y__val), yhatVal))\n",
        "        scores_MAEVal.append(mi_MAE(np.ravel(y__val), yhatVal))\n",
        "        scores_MAPEVal.append(mi_MAPE(np.ravel(y__val), yhatVal))\n",
        "        \n",
        "        res=mi_MAPE(np.ravel(y__val), yhatVal)\n",
        "        A[j,i]=res\n",
        "        j=j+1\n",
        "\n",
        " \n",
        "  # Desplegar información:\n",
        "    \n",
        "    print('>> %s:\\nRMSE: %.3f \\nMAE: %.3f  \\nMAPE: %.3f' % (nombres[i],\n",
        "                                                                 np.mean(scores_RMSEVal),\n",
        "                                                                 np.mean(scores_MAEVal),\n",
        "                                                                 np.mean(scores_MAPEVal)))\n",
        "\n",
        "  if i ==2:\n",
        "\n",
        "    pipeline=Pipeline(steps=[('ct',columnasTransformer),('m',modelos[i])])\n",
        "\n",
        "    for train_index, test_index in kf.split(X_test):\n",
        "\n",
        "      X__train, X__val = X_testt.loc[train_index], X_testt.loc[test_index]\n",
        "      y__train, y__val = y_testt.loc[train_index], y_testt.loc[test_index]\n",
        "\n",
        "      pipeline.fit(X__train,y__train)\n",
        "      yhatVal=pipeline.predict(X__val)\n",
        "\n",
        "      scores_RMSEVal.append(mi_RMSE(np.ravel(y__val), yhatVal))\n",
        "      scores_MAEVal.append(mi_MAE(np.ravel(y__val), yhatVal))\n",
        "      scores_MAPEVal.append(mi_MAPE(np.ravel(y__val), yhatVal))\n",
        "      print(mi_MAPE(np.ravel(y__val), yhatVal))\n",
        "\n",
        "  # Desplegar información:\n",
        "    \n",
        "    print('>> %s:\\nRMSE: %.3f \\nMAE: %.3f  \\nMAPE: %.3f' % (nombres[i],\n",
        "                                                                 np.mean(scores_RMSEVal),\n",
        "                                                                 np.mean(scores_MAEVal),\n",
        "                                                                 np.mean(scores_MAPEVal)))\n",
        "  "
      ],
      "metadata": {
        "id": "n0P_AcyjC_Dh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7f97a1e-d8ba-4702-f930-8293e8f986df"
      },
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_encoders.py:174: UserWarning: Found unknown categories in columns [2] during transform. These unknown categories will be encoded as all zeros\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> LLM:\n",
            "RMSE: 0.495 \n",
            "MAE: 0.319  \n",
            "MAPE: 6.188\n",
            ">> RF:\n",
            "RMSE: 0.343 \n",
            "MAE: 0.270  \n",
            "MAPE: 5.456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-7.**\n",
        "Obtener los diagramas de caja y bigote para los errores MAPE de los conjuntos de validación obtenidos. En particular compara estos primeros resultados de MAPE con el mejor resultado que encuentran los autores del artículo citado al inicio. Incluye tus conclusiones."
      ],
      "metadata": {
        "id": "iCNGx4TQ8CFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(rc={'figure.figsize':(8,4)})\n",
        "\n",
        "plt.boxplot(A,labels=nombres,showmeans=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ewvwUcJX78y1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "5d124de8-7625-42a6-b4a5-0e1e01de5780"
      },
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-8.**\n",
        "\n",
        "Usando una búsqueda de malla con validación cruzada (GridSearchCV), busca los mejores hiperparámetros para el modelo MLP. Al menos deberás realizar la búsqueda en los hiperparámetros “hidden_layer_sizes”, “alpha” y “learning_rate_init”. Además aplica la validación cruzada con repeticiones (RepeatedKFold). Muestra los mejores hiperparámetros encontrados."
      ],
      "metadata": {
        "id": "tzQn5NR78GFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "XtrainTf = columnasTransformer.fit(X_train)\n",
        "XtrainFTf = XtrainTf.transform(X_train)   \n",
        "XtestFTf = XtrainTf.transform(X_test)\n",
        "\n",
        "modelo_MLP=MLPClassifier(max_iter=3000)\n",
        "\n",
        "cv = RepeatedKFold(n_splits=5, n_repeats=3)\n",
        "grid_model_MLP = GridSearchCV(modelo_MLP,param_grid={\"alpha\": [0.0001, 0.001, 0.01, 0.1], \"hidden_layer_sizes\": [(10,30,10),(20,)], \"learning_rate_init\": np.linspace(0.00001, 1, 10)},cv=cv,scoring=make_scorer(mi_MAPE), n_jobs=-1)\n",
        "#\"hidden_layer_sizes\":np.arange(1,5),\n",
        "grid_model_MLP.fit(XtrainFTf,y_train)\n",
        "\n",
        "print(\"Utilizando la métrica\", grid_model_MLP.scoring, \"la mejor puntuación obtenida es:\", grid_model_MLP.best_score_)\n",
        "print(\"Los mejores hiperparámetros encontrados son:\", grid_model_MLP.best_params_)\n"
      ],
      "metadata": {
        "id": "FnuXHIJYH4yA",
        "outputId": "21664ca5-93f9-4b27-889a-2d8c6d8344c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "1200 fits failed out of a total of 1200.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "80 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 752, in fit\n",
            "    return self._fit(X, y, incremental=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 393, in _fit\n",
            "    X, y = self._validate_input(X, y, incremental, reset=first_pass)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 1131, in _validate_input\n",
            "    self._label_binarizer.fit(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py\", line 301, in fit\n",
            "    self.classes_ = unique_labels(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py\", line 101, in unique_labels\n",
            "    raise ValueError(\"Unknown label type: %s\" % repr(ys))\n",
            "ValueError: Unknown label type: (array([1.05198076, 1.10877032, 1.09342798, 1.08106251, 1.2441035 ,\n",
            "       1.11609801, 1.02518892, 1.13304328, 1.07098695, 1.06686512,\n",
            "       1.06411724, 1.05381269, 1.04076025, 1.01923517, 1.07465079,\n",
            "       1.20036638, 1.22647126, 1.04350813, 1.0604534 , 1.04236318,\n",
            "       1.08770323, 1.09319899, 1.05381269, 1.12525761, 1.03068468,\n",
            "       1.15754523, 1.08793222, 1.01144951, 1.09457293, 1.4000458 ,\n",
            "       1.10968628, 1.20563316, 1.18433707, 1.09983971, 1.30799176,\n",
            "       1.2466224 , 1.03801237, 1.11564003, 1.02999771, 1.06411724,\n",
            "       1.10487749, 1.05885047, 1.0883902 , 1.2603618 , 1.14220289,\n",
            "       1.16006412, 1.17128463, 1.10579345, 1.05930845, 1.10373254,\n",
            "       1.11380811, 1.02953973, 1.51957866, 1.04373712, 1.25166018,\n",
            "       1.1673918 , 1.21685368, 1.05541562, 1.18021525, 1.16326998,\n",
            "       1.00526677, 1.00549576, 1.07716968, 1.08701626, 1.05793451,\n",
            "       1.08106251, 1.07190291, 1.40531257, 1.45088161, 1.30272498,\n",
            "       1.06778109, 1.13464621, 1.14014197, 1.09022212, 1.11357912,\n",
            "       1.04030227, 1.75727044, 1.1348752 , 1.09411495, 1.22120449,\n",
            "       1.08197847, 1.10029769, 1.0977788 , 1.03595145, 1.1209068 ,\n",
            "       1.09800779, 1.10052668, 1.02610488, 1.07007099, 1.28028395,\n",
            "       1.0790016 , 1.29013052, 1.37829173, 1.06365926, 1.06388825,\n",
            "       1.04167621, 1.10121365, 1.05289673, 1.17242959, 1.07167392,\n",
            "       1.07739867, 1.78337531, 1.08541333, 1.31302954, 1.29379437,\n",
            "       1.03755439, 1.07304786, 1.07098695, 1.08884818, 1.0487749 ,\n",
            "       1.10350355, 1.03366155, 1.13144035, 1.03938631, 1.07098695,\n",
            "       1.19601557, 1.06091138, 1.07579574, 1.30776277, 1.35104191,\n",
            "       1.05724754, 1.05060682, 1.07968857, 1.03595145, 1.19853446,\n",
            "       1.03801237, 1.03366155, 1.16372796, 1.30844974, 1.09892375,\n",
            "       1.08037554, 1.86764369, 1.28990153, 1.0208381 , 1.10716739,\n",
            "       1.09686283, 1.05472865, 1.15800321, 1.10670941, 1.07281887,\n",
            "       1.08564232, 1.05472865, 1.01122052, 1.09663384, 1.15937715,\n",
            "       1.10579345, 1.07602473, 1.00137394, 1.07625372, 1.0767117 ,\n",
            "       1.06205633, 1.11060224, 1.11518205, 1.03640943, 1.0370964 ,\n",
            "       1.02060911, 1.10441951, 1.36730021, 1.08861919, 1.04488207,\n",
            "       1.35699565, 1.09480192, 1.09045111, 1.03572246, 1.07029998,\n",
            "       1.27845203, 1.25051523, 1.14815663, 1.06778109, 1.1069384 ,\n",
            "       1.12846348, 1.03389054, 1.09892375, 1.07923059, 1.22578429,\n",
            "       1.18731395, 1.12892146, 1.2905885 , 1.07281887, 1.07029998,\n",
            "       1.13144035, 1.46072819, 1.07419281, 1.1046485 , 1.11678498,\n",
            "       1.0581635 , 1.06182734, 1.48477215, 1.09274101, 1.13166934,\n",
            "       1.16487291, 1.03320357, 1.23150905, 1.01557133, 1.05266774,\n",
            "       1.08770323, 1.17197161, 1.08266545, 1.07831463, 1.04831692,\n",
            "       1.04236318, 1.09594687, 1.11128921, 1.06686512, 1.08037554,\n",
            "       1.31531944, 1.53858484, 1.15136249, 1.25005725, 1.04465308,\n",
            "       1.09823678, 1.14586673, 1.20861003, 1.069842  , 1.12960843,\n",
            "       1.12686054, 1.08312343, 1.01923517, 1.05129379, 1.03824136,\n",
            "       1.68811541, 1.14586673, 1.02931074, 1.09869476, 1.15456835,\n",
            "       1.04282116, 1.0558736 , 1.17838333, 1.08770323, 1.08152049,\n",
            "       1.08060453, 1.05564461, 1.26402565, 1.02404397, 1.0373254 ,\n",
            "       1.08403939, 1.10533547, 1.12983742, 1.07281887, 1.02106709,\n",
            "       1.05885047, 1.15960614, 1.16784978, 1.11174719, 1.41905198,\n",
            "       1.10602244, 1.10854133, 1.75360659, 1.07579574, 1.04831692,\n",
            "       1.09983971, 1.51454087, 1.07579574, 1.09663384, 1.93771468,\n",
            "       1.09113808, 1.06457522, 1.16670483, 1.07785665, 1.13624914,\n",
            "       1.14014197, 1.09022212, 1.08106251, 1.20952599, 1.04854591,\n",
            "       1.12502862, 1.10945729, 1.08587131, 1.09800779, 1.13373025,\n",
            "       1.11426609, 1.50583925, 1.12594458, 1.0838104 , 1.17151362,\n",
            "       1.08655828, 1.03984429, 1.05953744, 1.36523929, 1.13144035,\n",
            "       1.01900618, 1.08426838, 1.07487978, 1.22349439, 1.19097779,\n",
            "       1.35607969, 1.64048546, 1.1254866 , 1.1023586 , 1.01534234,\n",
            "       1.11174719, 1.08655828, 1.06457522, 1.02862377, 1.29722922,\n",
            "       1.04396611, 1.14838562, 1.08747424, 1.06617815, 1.0861003 ,\n",
            "       1.0022899 , 1.12594458, 1.23471491, 1.35928555, 1.44126403,\n",
            "       1.0861003 , 1.13395924, 1.07762766, 1.12823449, 1.11083123,\n",
            "       1.03274559, 1.06365926, 1.10327456, 1.10831234, 1.1000687 ,\n",
            "       1.05862148, 1.07373483, 1.08701626, 1.0396153 , 1.01557133]),)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "80 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 752, in fit\n",
            "    return self._fit(X, y, incremental=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 393, in _fit\n",
            "    X, y = self._validate_input(X, y, incremental, reset=first_pass)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 1131, in _validate_input\n",
            "    self._label_binarizer.fit(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py\", line 301, in fit\n",
            "    self.classes_ = unique_labels(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py\", line 101, in unique_labels\n",
            "    raise ValueError(\"Unknown label type: %s\" % repr(ys))\n",
            "ValueError: Unknown label type: (array([1.05198076, 1.10877032, 1.09342798, 1.08106251, 1.2441035 ,\n",
            "       1.06801008, 1.13304328, 1.06686512, 1.06411724, 1.05381269,\n",
            "       1.07762766, 1.04076025, 1.01923517, 1.20036638, 1.22647126,\n",
            "       1.04350813, 1.0604534 , 1.04236318, 1.08770323, 1.09319899,\n",
            "       1.05381269, 1.12525761, 1.12296771, 1.15754523, 1.08793222,\n",
            "       1.06869705, 1.01144951, 1.09457293, 1.4000458 , 1.10968628,\n",
            "       1.12686054, 1.18433707, 1.30799176, 1.2466224 , 1.02999771,\n",
            "       1.10487749, 1.05885047, 1.2603618 , 1.17128463, 1.09594687,\n",
            "       1.10579345, 1.09251202, 1.05930845, 1.10373254, 1.11380811,\n",
            "       1.02953973, 1.51957866, 1.04373712, 1.25166018, 1.1673918 ,\n",
            "       1.21685368, 1.05541562, 1.18021525, 1.10029769, 1.16326998,\n",
            "       1.00526677, 1.09457293, 1.00549576, 1.07716968, 1.08701626,\n",
            "       1.05793451, 1.05449966, 1.07190291, 1.14953057, 1.40531257,\n",
            "       1.12846348, 1.11266316, 1.06778109, 1.13464621, 1.14014197,\n",
            "       1.09022212, 1.75727044, 1.1348752 , 1.09411495, 1.22120449,\n",
            "       1.08197847, 1.10029769, 1.08976414, 1.1209068 , 1.09800779,\n",
            "       1.10052668, 1.02610488, 1.07007099, 1.28028395, 1.0790016 ,\n",
            "       1.37829173, 1.06365926, 1.06388825, 1.04167621, 1.15846119,\n",
            "       1.05289673, 1.07167392, 1.78337531, 1.06869705, 1.31302954,\n",
            "       1.03755439, 1.07304786, 1.08884818, 1.09136707, 1.0487749 ,\n",
            "       1.10350355, 1.0279368 , 1.03366155, 1.10396153, 1.03938631,\n",
            "       1.07098695, 1.19601557, 1.47858942, 1.06091138, 1.0510648 ,\n",
            "       1.30776277, 1.35104191, 1.05724754, 1.05060682, 1.03663842,\n",
            "       1.07968857, 1.03595145, 1.19853446, 1.08541333, 1.03801237,\n",
            "       1.16372796, 1.09205404, 1.30844974, 1.09892375, 1.08037554,\n",
            "       1.00183192, 1.05060682, 1.86764369, 1.09754981, 1.05678956,\n",
            "       1.28990153, 1.0208381 , 1.05999542, 1.05472865, 1.08197847,\n",
            "       1.31669338, 1.07281887, 1.08564232, 1.05472865, 1.01122052,\n",
            "       1.09663384, 1.15937715, 1.04350813, 1.53423403, 1.10579345,\n",
            "       1.07602473, 1.00137394, 1.1417449 , 1.07625372, 1.0767117 ,\n",
            "       1.06205633, 1.11060224, 1.11518205, 1.03640943, 1.0370964 ,\n",
            "       1.10441951, 1.07739867, 1.36730021, 1.08861919, 1.09045111,\n",
            "       1.04488207, 1.05907946, 1.35699565, 1.09480192, 1.09045111,\n",
            "       1.03572246, 1.07029998, 1.27845203, 1.08793222, 1.25051523,\n",
            "       1.1069384 , 1.03389054, 1.09892375, 1.11930387, 1.22578429,\n",
            "       1.18731395, 1.12892146, 1.2905885 , 1.07281887, 1.07029998,\n",
            "       1.46072819, 1.2349439 , 1.1046485 , 1.0581635 , 1.27272727,\n",
            "       1.06182734, 1.48477215, 1.14357683, 1.16487291, 1.26929242,\n",
            "       1.0767117 , 1.03320357, 1.01557133, 1.05266774, 1.09869476,\n",
            "       1.17197161, 1.04831692, 1.03915732, 1.09136707, 1.04236318,\n",
            "       1.14678269, 1.09594687, 1.11128921, 1.06686512, 1.08037554,\n",
            "       1.31531944, 1.15136249, 1.25005725, 1.07831463, 1.04465308,\n",
            "       1.09823678, 1.20861003, 1.069842  , 1.12686054, 1.08312343,\n",
            "       1.15273643, 1.05129379, 1.03824136, 1.68811541, 1.02931074,\n",
            "       1.04007328, 1.15456835, 1.04282116, 1.0558736 , 1.00137394,\n",
            "       1.08907717, 1.0977788 , 1.08152049, 1.08060453, 1.02404397,\n",
            "       1.0373254 , 1.08403939, 1.10533547, 1.12983742, 1.02106709,\n",
            "       1.16784978, 1.11174719, 1.11357912, 1.10854133, 1.75360659,\n",
            "       1.07579574, 1.98671857, 1.04831692, 1.09983971, 1.1813602 ,\n",
            "       1.08999313, 1.20059537, 1.09663384, 1.05472865, 1.09113808,\n",
            "       1.06457522, 1.16670483, 1.05930845, 1.13624914, 1.14014197,\n",
            "       1.09022212, 1.08060453, 1.08106251, 1.20952599, 1.09594687,\n",
            "       1.10327456, 1.04854591, 1.12502862, 1.10945729, 1.09800779,\n",
            "       1.13373025, 1.09846577, 1.12594458, 1.0838104 , 1.17151362,\n",
            "       1.03984429, 1.09022212, 1.05953744, 1.36523929, 1.13144035,\n",
            "       1.01900618, 1.08426838, 1.07487978, 1.19097779, 1.08472636,\n",
            "       1.0767117 , 1.35607969, 1.64048546, 1.16326998, 1.1254866 ,\n",
            "       1.1023586 , 1.01534234, 1.11174719, 1.08655828, 1.15937715,\n",
            "       1.06457522, 1.29722922, 1.0487749 , 1.08747424, 1.06617815,\n",
            "       1.0861003 , 1.12067781, 1.0022899 , 1.06801008, 1.23471491,\n",
            "       1.51362491, 1.35928555, 1.07694069, 2.        , 1.44126403,\n",
            "       1.0861003 , 1.13395924, 1.07762766, 1.03274559, 1.06365926,\n",
            "       1.10327456, 1.1000687 , 1.45019464, 1.04762995, 1.05862148,\n",
            "       1.08701626, 1.0838104 , 1.0396153 , 1.06663613, 1.01557133]),)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "80 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 752, in fit\n",
            "    return self._fit(X, y, incremental=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 393, in _fit\n",
            "    X, y = self._validate_input(X, y, incremental, reset=first_pass)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 1131, in _validate_input\n",
            "    self._label_binarizer.fit(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py\", line 301, in fit\n",
            "    self.classes_ = unique_labels(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py\", line 101, in unique_labels\n",
            "    raise ValueError(\"Unknown label type: %s\" % repr(ys))\n",
            "ValueError: Unknown label type: (array([1.05198076, 1.10877032, 1.09342798, 1.08106251, 1.2441035 ,\n",
            "       1.11609801, 1.06801008, 1.02518892, 1.13304328, 1.07098695,\n",
            "       1.06686512, 1.05381269, 1.07762766, 1.04076025, 1.01923517,\n",
            "       1.07465079, 1.20036638, 1.04350813, 1.0604534 , 1.04236318,\n",
            "       1.09319899, 1.05381269, 1.12525761, 1.12296771, 1.03068468,\n",
            "       1.15754523, 1.08793222, 1.06869705, 1.01144951, 1.09457293,\n",
            "       1.4000458 , 1.10968628, 1.12686054, 1.20563316, 1.09983971,\n",
            "       1.30799176, 1.2466224 , 1.03801237, 1.11564003, 1.06411724,\n",
            "       1.10487749, 1.05885047, 1.0883902 , 1.2603618 , 1.14220289,\n",
            "       1.16006412, 1.09594687, 1.09251202, 1.51957866, 1.04373712,\n",
            "       1.25166018, 1.1673918 , 1.21685368, 1.10029769, 1.16326998,\n",
            "       1.00526677, 1.09457293, 1.00549576, 1.08701626, 1.08106251,\n",
            "       1.05449966, 1.07190291, 1.14953057, 1.45088161, 1.12846348,\n",
            "       1.11266316, 1.30272498, 1.13464621, 1.14014197, 1.11357912,\n",
            "       1.04030227, 1.75727044, 1.22120449, 1.08197847, 1.10029769,\n",
            "       1.0977788 , 1.03595145, 1.08976414, 1.09800779, 1.02610488,\n",
            "       1.28028395, 1.0790016 , 1.29013052, 1.37829173, 1.06388825,\n",
            "       1.04167621, 1.15846119, 1.10121365, 1.17242959, 1.07739867,\n",
            "       1.08541333, 1.06869705, 1.31302954, 1.29379437, 1.07304786,\n",
            "       1.07098695, 1.08884818, 1.09136707, 1.0487749 , 1.0279368 ,\n",
            "       1.03366155, 1.13144035, 1.10396153, 1.03938631, 1.07098695,\n",
            "       1.19601557, 1.47858942, 1.0510648 , 1.07579574, 1.35104191,\n",
            "       1.05724754, 1.03663842, 1.07968857, 1.03595145, 1.19853446,\n",
            "       1.08541333, 1.03801237, 1.03366155, 1.16372796, 1.09205404,\n",
            "       1.30844974, 1.09892375, 1.08037554, 1.00183192, 1.05060682,\n",
            "       1.09754981, 1.05678956, 1.28990153, 1.0208381 , 1.10716739,\n",
            "       1.09686283, 1.05999542, 1.15800321, 1.08197847, 1.10670941,\n",
            "       1.31669338, 1.07281887, 1.05472865, 1.09663384, 1.15937715,\n",
            "       1.04350813, 1.53423403, 1.07602473, 1.1417449 , 1.07625372,\n",
            "       1.0767117 , 1.06205633, 1.11060224, 1.02060911, 1.10441951,\n",
            "       1.07739867, 1.36730021, 1.09045111, 1.04488207, 1.05907946,\n",
            "       1.35699565, 1.03572246, 1.08793222, 1.25051523, 1.14815663,\n",
            "       1.06778109, 1.1069384 , 1.12846348, 1.03389054, 1.09892375,\n",
            "       1.11930387, 1.07923059, 1.18731395, 1.12892146, 1.07281887,\n",
            "       1.13144035, 1.46072819, 1.07419281, 1.2349439 , 1.11678498,\n",
            "       1.0581635 , 1.27272727, 1.06182734, 1.09274101, 1.14357683,\n",
            "       1.13166934, 1.16487291, 1.26929242, 1.0767117 , 1.03320357,\n",
            "       1.23150905, 1.01557133, 1.05266774, 1.08770323, 1.09869476,\n",
            "       1.17197161, 1.08266545, 1.07831463, 1.04831692, 1.03915732,\n",
            "       1.09136707, 1.14678269, 1.09594687, 1.11128921, 1.06686512,\n",
            "       1.08037554, 1.31531944, 1.53858484, 1.07831463, 1.04465308,\n",
            "       1.14586673, 1.12960843, 1.08312343, 1.01923517, 1.15273643,\n",
            "       1.05129379, 1.03824136, 1.14586673, 1.04007328, 1.09869476,\n",
            "       1.15456835, 1.00137394, 1.17838333, 1.08907717, 1.08770323,\n",
            "       1.0977788 , 1.08152049, 1.08060453, 1.05564461, 1.26402565,\n",
            "       1.02404397, 1.0373254 , 1.07281887, 1.02106709, 1.05885047,\n",
            "       1.15960614, 1.16784978, 1.11174719, 1.11357912, 1.41905198,\n",
            "       1.10602244, 1.07579574, 1.98671857, 1.04831692, 1.09983971,\n",
            "       1.1813602 , 1.51454087, 1.07579574, 1.08999313, 1.20059537,\n",
            "       1.09663384, 1.05472865, 1.93771468, 1.09113808, 1.06457522,\n",
            "       1.07785665, 1.05930845, 1.14014197, 1.08060453, 1.08106251,\n",
            "       1.09594687, 1.10327456, 1.04854591, 1.12502862, 1.10945729,\n",
            "       1.08587131, 1.09800779, 1.11426609, 1.09846577, 1.50583925,\n",
            "       1.12594458, 1.0838104 , 1.08655828, 1.03984429, 1.09022212,\n",
            "       1.36523929, 1.13144035, 1.01900618, 1.07487978, 1.22349439,\n",
            "       1.19097779, 1.08472636, 1.0767117 , 1.35607969, 1.64048546,\n",
            "       1.16326998, 1.1254866 , 1.1023586 , 1.01534234, 1.11174719,\n",
            "       1.08655828, 1.15937715, 1.06457522, 1.02862377, 1.04396611,\n",
            "       1.14838562, 1.0487749 , 1.0861003 , 1.12067781, 1.0022899 ,\n",
            "       1.12594458, 1.06801008, 1.23471491, 1.51362491, 1.35928555,\n",
            "       1.07694069, 2.        , 1.44126403, 1.07762766, 1.12823449,\n",
            "       1.11083123, 1.03274559, 1.06365926, 1.10327456, 1.10831234,\n",
            "       1.1000687 , 1.45019464, 1.04762995, 1.05862148, 1.07373483,\n",
            "       1.08701626, 1.0838104 , 1.0396153 , 1.06663613, 1.01557133]),)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "80 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 752, in fit\n",
            "    return self._fit(X, y, incremental=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 393, in _fit\n",
            "    X, y = self._validate_input(X, y, incremental, reset=first_pass)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 1131, in _validate_input\n",
            "    self._label_binarizer.fit(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py\", line 301, in fit\n",
            "    self.classes_ = unique_labels(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py\", line 101, in unique_labels\n",
            "    raise ValueError(\"Unknown label type: %s\" % repr(ys))\n",
            "ValueError: Unknown label type: (array([1.10877032, 1.09342798, 1.08106251, 1.2441035 , 1.11609801,\n",
            "       1.06801008, 1.02518892, 1.13304328, 1.07098695, 1.06411724,\n",
            "       1.07762766, 1.04076025, 1.07465079, 1.22647126, 1.0604534 ,\n",
            "       1.08770323, 1.09319899, 1.12296771, 1.03068468, 1.15754523,\n",
            "       1.06869705, 1.01144951, 1.10968628, 1.12686054, 1.20563316,\n",
            "       1.18433707, 1.09983971, 1.2466224 , 1.03801237, 1.11564003,\n",
            "       1.02999771, 1.06411724, 1.10487749, 1.05885047, 1.0883902 ,\n",
            "       1.14220289, 1.16006412, 1.17128463, 1.09594687, 1.10579345,\n",
            "       1.09251202, 1.05930845, 1.10373254, 1.11380811, 1.02953973,\n",
            "       1.04373712, 1.21685368, 1.05541562, 1.18021525, 1.10029769,\n",
            "       1.16326998, 1.00526677, 1.09457293, 1.07716968, 1.05793451,\n",
            "       1.08106251, 1.05449966, 1.07190291, 1.14953057, 1.40531257,\n",
            "       1.45088161, 1.12846348, 1.11266316, 1.30272498, 1.06778109,\n",
            "       1.13464621, 1.14014197, 1.09022212, 1.11357912, 1.04030227,\n",
            "       1.1348752 , 1.09411495, 1.22120449, 1.0977788 , 1.03595145,\n",
            "       1.08976414, 1.1209068 , 1.09800779, 1.10052668, 1.07007099,\n",
            "       1.28028395, 1.29013052, 1.37829173, 1.06365926, 1.04167621,\n",
            "       1.15846119, 1.10121365, 1.05289673, 1.17242959, 1.07167392,\n",
            "       1.07739867, 1.78337531, 1.08541333, 1.06869705, 1.31302954,\n",
            "       1.29379437, 1.03755439, 1.07304786, 1.07098695, 1.08884818,\n",
            "       1.09136707, 1.0487749 , 1.10350355, 1.0279368 , 1.03366155,\n",
            "       1.13144035, 1.10396153, 1.19601557, 1.47858942, 1.06091138,\n",
            "       1.0510648 , 1.07579574, 1.30776277, 1.05724754, 1.05060682,\n",
            "       1.03663842, 1.07968857, 1.03595145, 1.19853446, 1.08541333,\n",
            "       1.03366155, 1.09205404, 1.08037554, 1.00183192, 1.05060682,\n",
            "       1.86764369, 1.09754981, 1.05678956, 1.10716739, 1.09686283,\n",
            "       1.05999542, 1.05472865, 1.15800321, 1.08197847, 1.10670941,\n",
            "       1.31669338, 1.08564232, 1.01122052, 1.15937715, 1.04350813,\n",
            "       1.53423403, 1.10579345, 1.07602473, 1.00137394, 1.1417449 ,\n",
            "       1.06205633, 1.11518205, 1.03640943, 1.0370964 , 1.02060911,\n",
            "       1.10441951, 1.07739867, 1.36730021, 1.08861919, 1.09045111,\n",
            "       1.05907946, 1.35699565, 1.09480192, 1.09045111, 1.03572246,\n",
            "       1.07029998, 1.27845203, 1.08793222, 1.25051523, 1.14815663,\n",
            "       1.06778109, 1.12846348, 1.03389054, 1.11930387, 1.07923059,\n",
            "       1.22578429, 1.18731395, 1.2905885 , 1.07281887, 1.07029998,\n",
            "       1.13144035, 1.46072819, 1.07419281, 1.2349439 , 1.1046485 ,\n",
            "       1.11678498, 1.0581635 , 1.27272727, 1.48477215, 1.09274101,\n",
            "       1.14357683, 1.13166934, 1.26929242, 1.0767117 , 1.03320357,\n",
            "       1.23150905, 1.01557133, 1.08770323, 1.09869476, 1.17197161,\n",
            "       1.08266545, 1.07831463, 1.03915732, 1.09136707, 1.04236318,\n",
            "       1.14678269, 1.09594687, 1.11128921, 1.08037554, 1.31531944,\n",
            "       1.53858484, 1.15136249, 1.25005725, 1.07831463, 1.04465308,\n",
            "       1.09823678, 1.14586673, 1.20861003, 1.069842  , 1.12960843,\n",
            "       1.12686054, 1.01923517, 1.15273643, 1.03824136, 1.68811541,\n",
            "       1.14586673, 1.02931074, 1.04007328, 1.09869476, 1.04282116,\n",
            "       1.0558736 , 1.00137394, 1.17838333, 1.08907717, 1.08770323,\n",
            "       1.0977788 , 1.05564461, 1.26402565, 1.08403939, 1.10533547,\n",
            "       1.12983742, 1.07281887, 1.05885047, 1.15960614, 1.16784978,\n",
            "       1.11174719, 1.11357912, 1.41905198, 1.10602244, 1.10854133,\n",
            "       1.75360659, 1.98671857, 1.04831692, 1.09983971, 1.1813602 ,\n",
            "       1.51454087, 1.07579574, 1.08999313, 1.20059537, 1.05472865,\n",
            "       1.93771468, 1.09113808, 1.06457522, 1.16670483, 1.07785665,\n",
            "       1.05930845, 1.13624914, 1.09022212, 1.08060453, 1.08106251,\n",
            "       1.20952599, 1.09594687, 1.10327456, 1.04854591, 1.10945729,\n",
            "       1.08587131, 1.13373025, 1.11426609, 1.09846577, 1.50583925,\n",
            "       1.12594458, 1.17151362, 1.08655828, 1.09022212, 1.05953744,\n",
            "       1.13144035, 1.08426838, 1.22349439, 1.08472636, 1.0767117 ,\n",
            "       1.16326998, 1.1254866 , 1.01534234, 1.08655828, 1.15937715,\n",
            "       1.02862377, 1.29722922, 1.04396611, 1.14838562, 1.0487749 ,\n",
            "       1.08747424, 1.06617815, 1.0861003 , 1.12067781, 1.0022899 ,\n",
            "       1.12594458, 1.06801008, 1.23471491, 1.51362491, 1.35928555,\n",
            "       1.07694069, 2.        , 1.0861003 , 1.13395924, 1.12823449,\n",
            "       1.11083123, 1.03274559, 1.10831234, 1.45019464, 1.04762995,\n",
            "       1.07373483, 1.08701626, 1.0838104 , 1.0396153 , 1.06663613]),)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "80 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 752, in fit\n",
            "    return self._fit(X, y, incremental=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 393, in _fit\n",
            "    X, y = self._validate_input(X, y, incremental, reset=first_pass)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 1131, in _validate_input\n",
            "    self._label_binarizer.fit(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py\", line 301, in fit\n",
            "    self.classes_ = unique_labels(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py\", line 101, in unique_labels\n",
            "    raise ValueError(\"Unknown label type: %s\" % repr(ys))\n",
            "ValueError: Unknown label type: (array([1.05198076, 1.11609801, 1.06801008, 1.02518892, 1.07098695,\n",
            "       1.06686512, 1.06411724, 1.05381269, 1.07762766, 1.01923517,\n",
            "       1.07465079, 1.20036638, 1.22647126, 1.04350813, 1.04236318,\n",
            "       1.08770323, 1.05381269, 1.12525761, 1.12296771, 1.03068468,\n",
            "       1.08793222, 1.06869705, 1.09457293, 1.4000458 , 1.12686054,\n",
            "       1.20563316, 1.18433707, 1.09983971, 1.30799176, 1.03801237,\n",
            "       1.11564003, 1.02999771, 1.06411724, 1.0883902 , 1.2603618 ,\n",
            "       1.14220289, 1.16006412, 1.17128463, 1.09594687, 1.10579345,\n",
            "       1.09251202, 1.05930845, 1.10373254, 1.11380811, 1.02953973,\n",
            "       1.51957866, 1.25166018, 1.1673918 , 1.05541562, 1.18021525,\n",
            "       1.10029769, 1.09457293, 1.00549576, 1.07716968, 1.08701626,\n",
            "       1.05793451, 1.08106251, 1.05449966, 1.14953057, 1.40531257,\n",
            "       1.45088161, 1.12846348, 1.11266316, 1.30272498, 1.06778109,\n",
            "       1.09022212, 1.11357912, 1.04030227, 1.75727044, 1.1348752 ,\n",
            "       1.09411495, 1.08197847, 1.10029769, 1.0977788 , 1.03595145,\n",
            "       1.08976414, 1.1209068 , 1.10052668, 1.02610488, 1.07007099,\n",
            "       1.0790016 , 1.29013052, 1.06365926, 1.06388825, 1.15846119,\n",
            "       1.10121365, 1.05289673, 1.17242959, 1.07167392, 1.07739867,\n",
            "       1.78337531, 1.08541333, 1.06869705, 1.29379437, 1.03755439,\n",
            "       1.07098695, 1.09136707, 1.10350355, 1.0279368 , 1.13144035,\n",
            "       1.10396153, 1.03938631, 1.07098695, 1.47858942, 1.06091138,\n",
            "       1.0510648 , 1.07579574, 1.30776277, 1.35104191, 1.05060682,\n",
            "       1.03663842, 1.08541333, 1.03801237, 1.03366155, 1.16372796,\n",
            "       1.09205404, 1.30844974, 1.09892375, 1.00183192, 1.05060682,\n",
            "       1.86764369, 1.09754981, 1.05678956, 1.28990153, 1.0208381 ,\n",
            "       1.10716739, 1.09686283, 1.05999542, 1.05472865, 1.15800321,\n",
            "       1.08197847, 1.10670941, 1.31669338, 1.07281887, 1.08564232,\n",
            "       1.05472865, 1.01122052, 1.09663384, 1.04350813, 1.53423403,\n",
            "       1.10579345, 1.00137394, 1.1417449 , 1.07625372, 1.0767117 ,\n",
            "       1.11060224, 1.11518205, 1.03640943, 1.0370964 , 1.02060911,\n",
            "       1.07739867, 1.08861919, 1.09045111, 1.04488207, 1.05907946,\n",
            "       1.09480192, 1.09045111, 1.07029998, 1.27845203, 1.08793222,\n",
            "       1.14815663, 1.06778109, 1.1069384 , 1.12846348, 1.09892375,\n",
            "       1.11930387, 1.07923059, 1.22578429, 1.12892146, 1.2905885 ,\n",
            "       1.07029998, 1.13144035, 1.07419281, 1.2349439 , 1.1046485 ,\n",
            "       1.11678498, 1.27272727, 1.06182734, 1.48477215, 1.09274101,\n",
            "       1.14357683, 1.13166934, 1.16487291, 1.26929242, 1.0767117 ,\n",
            "       1.23150905, 1.05266774, 1.08770323, 1.09869476, 1.08266545,\n",
            "       1.07831463, 1.04831692, 1.03915732, 1.09136707, 1.04236318,\n",
            "       1.14678269, 1.06686512, 1.53858484, 1.15136249, 1.25005725,\n",
            "       1.07831463, 1.09823678, 1.14586673, 1.20861003, 1.069842  ,\n",
            "       1.12960843, 1.12686054, 1.08312343, 1.01923517, 1.15273643,\n",
            "       1.05129379, 1.68811541, 1.14586673, 1.02931074, 1.04007328,\n",
            "       1.09869476, 1.15456835, 1.04282116, 1.0558736 , 1.00137394,\n",
            "       1.17838333, 1.08907717, 1.08770323, 1.0977788 , 1.08152049,\n",
            "       1.08060453, 1.05564461, 1.26402565, 1.02404397, 1.0373254 ,\n",
            "       1.08403939, 1.10533547, 1.12983742, 1.07281887, 1.02106709,\n",
            "       1.05885047, 1.15960614, 1.11357912, 1.41905198, 1.10602244,\n",
            "       1.10854133, 1.75360659, 1.07579574, 1.98671857, 1.1813602 ,\n",
            "       1.51454087, 1.07579574, 1.08999313, 1.20059537, 1.09663384,\n",
            "       1.05472865, 1.93771468, 1.16670483, 1.07785665, 1.05930845,\n",
            "       1.13624914, 1.14014197, 1.09022212, 1.08060453, 1.20952599,\n",
            "       1.09594687, 1.10327456, 1.12502862, 1.08587131, 1.09800779,\n",
            "       1.13373025, 1.11426609, 1.09846577, 1.50583925, 1.0838104 ,\n",
            "       1.17151362, 1.08655828, 1.03984429, 1.09022212, 1.05953744,\n",
            "       1.36523929, 1.01900618, 1.08426838, 1.07487978, 1.22349439,\n",
            "       1.19097779, 1.08472636, 1.0767117 , 1.35607969, 1.64048546,\n",
            "       1.16326998, 1.1023586 , 1.11174719, 1.15937715, 1.06457522,\n",
            "       1.02862377, 1.29722922, 1.04396611, 1.14838562, 1.0487749 ,\n",
            "       1.08747424, 1.06617815, 1.12067781, 1.12594458, 1.06801008,\n",
            "       1.51362491, 1.07694069, 2.        , 1.44126403, 1.0861003 ,\n",
            "       1.13395924, 1.07762766, 1.12823449, 1.11083123, 1.06365926,\n",
            "       1.10327456, 1.10831234, 1.1000687 , 1.45019464, 1.04762995,\n",
            "       1.05862148, 1.07373483, 1.0838104 , 1.06663613, 1.01557133]),)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "80 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 752, in fit\n",
            "    return self._fit(X, y, incremental=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 393, in _fit\n",
            "    X, y = self._validate_input(X, y, incremental, reset=first_pass)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 1131, in _validate_input\n",
            "    self._label_binarizer.fit(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py\", line 301, in fit\n",
            "    self.classes_ = unique_labels(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py\", line 101, in unique_labels\n",
            "    raise ValueError(\"Unknown label type: %s\" % repr(ys))\n",
            "ValueError: Unknown label type: (array([1.10877032, 1.09342798, 1.08106251, 1.11609801, 1.06801008,\n",
            "       1.02518892, 1.13304328, 1.07098695, 1.06686512, 1.06411724,\n",
            "       1.05381269, 1.07762766, 1.01923517, 1.20036638, 1.22647126,\n",
            "       1.04350813, 1.04236318, 1.08770323, 1.05381269, 1.12525761,\n",
            "       1.03068468, 1.15754523, 1.08793222, 1.06869705, 1.01144951,\n",
            "       1.09457293, 1.10968628, 1.18433707, 1.09983971, 1.30799176,\n",
            "       1.2466224 , 1.03801237, 1.11564003, 1.02999771, 1.06411724,\n",
            "       1.10487749, 1.05885047, 1.0883902 , 1.2603618 , 1.14220289,\n",
            "       1.16006412, 1.17128463, 1.09594687, 1.10579345, 1.09251202,\n",
            "       1.05930845, 1.10373254, 1.11380811, 1.02953973, 1.04373712,\n",
            "       1.25166018, 1.1673918 , 1.21685368, 1.05541562, 1.18021525,\n",
            "       1.10029769, 1.16326998, 1.00526677, 1.00549576, 1.07716968,\n",
            "       1.08701626, 1.05793451, 1.05449966, 1.07190291, 1.45088161,\n",
            "       1.12846348, 1.11266316, 1.30272498, 1.06778109, 1.13464621,\n",
            "       1.14014197, 1.11357912, 1.04030227, 1.1348752 , 1.09411495,\n",
            "       1.22120449, 1.08197847, 1.10029769, 1.0977788 , 1.03595145,\n",
            "       1.1209068 , 1.09800779, 1.10052668, 1.02610488, 1.07007099,\n",
            "       1.28028395, 1.0790016 , 1.37829173, 1.06365926, 1.06388825,\n",
            "       1.04167621, 1.15846119, 1.10121365, 1.05289673, 1.17242959,\n",
            "       1.07739867, 1.78337531, 1.06869705, 1.31302954, 1.29379437,\n",
            "       1.03755439, 1.07098695, 1.08884818, 1.09136707, 1.0487749 ,\n",
            "       1.10350355, 1.0279368 , 1.03366155, 1.13144035, 1.03938631,\n",
            "       1.07098695, 1.19601557, 1.47858942, 1.06091138, 1.07579574,\n",
            "       1.30776277, 1.05724754, 1.05060682, 1.03663842, 1.07968857,\n",
            "       1.03595145, 1.19853446, 1.03366155, 1.16372796, 1.09205404,\n",
            "       1.30844974, 1.09892375, 1.86764369, 1.09754981, 1.05678956,\n",
            "       1.0208381 , 1.05999542, 1.05472865, 1.08197847, 1.31669338,\n",
            "       1.07281887, 1.05472865, 1.01122052, 1.09663384, 1.04350813,\n",
            "       1.53423403, 1.10579345, 1.07602473, 1.1417449 , 1.07625372,\n",
            "       1.0767117 , 1.06205633, 1.11518205, 1.03640943, 1.0370964 ,\n",
            "       1.02060911, 1.10441951, 1.07739867, 1.36730021, 1.08861919,\n",
            "       1.09045111, 1.04488207, 1.35699565, 1.09480192, 1.03572246,\n",
            "       1.07029998, 1.27845203, 1.14815663, 1.06778109, 1.1069384 ,\n",
            "       1.12846348, 1.03389054, 1.09892375, 1.11930387, 1.07923059,\n",
            "       1.22578429, 1.18731395, 1.12892146, 1.2905885 , 1.07281887,\n",
            "       1.07029998, 1.13144035, 1.46072819, 1.07419281, 1.2349439 ,\n",
            "       1.1046485 , 1.11678498, 1.0581635 , 1.27272727, 1.06182734,\n",
            "       1.48477215, 1.09274101, 1.13166934, 1.16487291, 1.26929242,\n",
            "       1.0767117 , 1.03320357, 1.23150905, 1.01557133, 1.09869476,\n",
            "       1.17197161, 1.08266545, 1.07831463, 1.04831692, 1.03915732,\n",
            "       1.04236318, 1.14678269, 1.09594687, 1.11128921, 1.08037554,\n",
            "       1.53858484, 1.15136249, 1.25005725, 1.04465308, 1.09823678,\n",
            "       1.14586673, 1.20861003, 1.12960843, 1.12686054, 1.08312343,\n",
            "       1.15273643, 1.05129379, 1.03824136, 1.68811541, 1.14586673,\n",
            "       1.02931074, 1.15456835, 1.0558736 , 1.00137394, 1.17838333,\n",
            "       1.08907717, 1.08770323, 1.0977788 , 1.08152049, 1.08060453,\n",
            "       1.05564461, 1.02404397, 1.0373254 , 1.08403939, 1.10533547,\n",
            "       1.12983742, 1.07281887, 1.05885047, 1.15960614, 1.16784978,\n",
            "       1.11174719, 1.10602244, 1.10854133, 1.75360659, 1.07579574,\n",
            "       1.98671857, 1.09983971, 1.1813602 , 1.51454087, 1.07579574,\n",
            "       1.08999313, 1.05472865, 1.93771468, 1.09113808, 1.06457522,\n",
            "       1.07785665, 1.05930845, 1.13624914, 1.09022212, 1.08060453,\n",
            "       1.08106251, 1.09594687, 1.04854591, 1.12502862, 1.10945729,\n",
            "       1.08587131, 1.09800779, 1.11426609, 1.09846577, 1.12594458,\n",
            "       1.17151362, 1.08655828, 1.03984429, 1.09022212, 1.05953744,\n",
            "       1.36523929, 1.13144035, 1.01900618, 1.08426838, 1.07487978,\n",
            "       1.22349439, 1.08472636, 1.0767117 , 1.35607969, 1.1023586 ,\n",
            "       1.01534234, 1.11174719, 1.08655828, 1.15937715, 1.06457522,\n",
            "       1.02862377, 1.29722922, 1.04396611, 1.14838562, 1.0487749 ,\n",
            "       1.06617815, 1.0861003 , 1.12067781, 1.0022899 , 1.12594458,\n",
            "       1.06801008, 1.51362491, 1.35928555, 1.07694069, 2.        ,\n",
            "       1.44126403, 1.07762766, 1.12823449, 1.03274559, 1.06365926,\n",
            "       1.10327456, 1.10831234, 1.1000687 , 1.45019464, 1.04762995,\n",
            "       1.05862148, 1.07373483, 1.08701626, 1.06663613, 1.01557133]),)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "80 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 752, in fit\n",
            "    return self._fit(X, y, incremental=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 393, in _fit\n",
            "    X, y = self._validate_input(X, y, incremental, reset=first_pass)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 1131, in _validate_input\n",
            "    self._label_binarizer.fit(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py\", line 301, in fit\n",
            "    self.classes_ = unique_labels(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py\", line 101, in unique_labels\n",
            "    raise ValueError(\"Unknown label type: %s\" % repr(ys))\n",
            "ValueError: Unknown label type: (array([1.05198076, 1.10877032, 1.09342798, 1.08106251, 1.2441035 ,\n",
            "       1.11609801, 1.06801008, 1.02518892, 1.13304328, 1.07098695,\n",
            "       1.06686512, 1.06411724, 1.05381269, 1.07762766, 1.04076025,\n",
            "       1.01923517, 1.07465079, 1.20036638, 1.22647126, 1.0604534 ,\n",
            "       1.04236318, 1.08770323, 1.09319899, 1.05381269, 1.12525761,\n",
            "       1.12296771, 1.03068468, 1.08793222, 1.06869705, 1.09457293,\n",
            "       1.4000458 , 1.12686054, 1.20563316, 1.18433707, 1.30799176,\n",
            "       1.2466224 , 1.03801237, 1.11564003, 1.02999771, 1.10487749,\n",
            "       1.05885047, 1.0883902 , 1.2603618 , 1.16006412, 1.17128463,\n",
            "       1.09594687, 1.10579345, 1.09251202, 1.05930845, 1.10373254,\n",
            "       1.11380811, 1.02953973, 1.51957866, 1.04373712, 1.25166018,\n",
            "       1.1673918 , 1.21685368, 1.18021525, 1.10029769, 1.00526677,\n",
            "       1.09457293, 1.00549576, 1.07716968, 1.08701626, 1.08106251,\n",
            "       1.05449966, 1.07190291, 1.14953057, 1.40531257, 1.45088161,\n",
            "       1.11266316, 1.30272498, 1.06778109, 1.14014197, 1.09022212,\n",
            "       1.11357912, 1.75727044, 1.1348752 , 1.09411495, 1.22120449,\n",
            "       1.08197847, 1.10029769, 1.0977788 , 1.08976414, 1.1209068 ,\n",
            "       1.10052668, 1.02610488, 1.07007099, 1.29013052, 1.06388825,\n",
            "       1.15846119, 1.10121365, 1.17242959, 1.07167392, 1.07739867,\n",
            "       1.78337531, 1.08541333, 1.29379437, 1.07304786, 1.07098695,\n",
            "       1.09136707, 1.10350355, 1.10396153, 1.03938631, 1.07098695,\n",
            "       1.19601557, 1.47858942, 1.06091138, 1.0510648 , 1.30776277,\n",
            "       1.35104191, 1.05724754, 1.05060682, 1.03663842, 1.19853446,\n",
            "       1.08541333, 1.03801237, 1.03366155, 1.16372796, 1.09205404,\n",
            "       1.30844974, 1.09892375, 1.08037554, 1.00183192, 1.05060682,\n",
            "       1.86764369, 1.09754981, 1.05678956, 1.28990153, 1.0208381 ,\n",
            "       1.10716739, 1.09686283, 1.05999542, 1.05472865, 1.15800321,\n",
            "       1.10670941, 1.31669338, 1.07281887, 1.08564232, 1.05472865,\n",
            "       1.01122052, 1.09663384, 1.15937715, 1.10579345, 1.00137394,\n",
            "       1.1417449 , 1.07625372, 1.06205633, 1.11060224, 1.11518205,\n",
            "       1.0370964 , 1.02060911, 1.10441951, 1.07739867, 1.36730021,\n",
            "       1.08861919, 1.09045111, 1.05907946, 1.09480192, 1.09045111,\n",
            "       1.03572246, 1.08793222, 1.25051523, 1.14815663, 1.06778109,\n",
            "       1.1069384 , 1.12846348, 1.03389054, 1.09892375, 1.11930387,\n",
            "       1.18731395, 1.2905885 , 1.07281887, 1.07029998, 1.13144035,\n",
            "       1.46072819, 1.2349439 , 1.1046485 , 1.11678498, 1.0581635 ,\n",
            "       1.27272727, 1.48477215, 1.09274101, 1.14357683, 1.13166934,\n",
            "       1.16487291, 1.26929242, 1.0767117 , 1.23150905, 1.01557133,\n",
            "       1.05266774, 1.08770323, 1.17197161, 1.08266545, 1.04831692,\n",
            "       1.03915732, 1.09136707, 1.04236318, 1.14678269, 1.09594687,\n",
            "       1.11128921, 1.06686512, 1.31531944, 1.07831463, 1.04465308,\n",
            "       1.09823678, 1.14586673, 1.069842  , 1.12960843, 1.08312343,\n",
            "       1.01923517, 1.15273643, 1.05129379, 1.03824136, 1.68811541,\n",
            "       1.02931074, 1.04007328, 1.09869476, 1.15456835, 1.04282116,\n",
            "       1.0558736 , 1.00137394, 1.17838333, 1.08907717, 1.08770323,\n",
            "       1.0977788 , 1.08152049, 1.05564461, 1.26402565, 1.02404397,\n",
            "       1.0373254 , 1.10533547, 1.12983742, 1.07281887, 1.02106709,\n",
            "       1.05885047, 1.15960614, 1.16784978, 1.11174719, 1.11357912,\n",
            "       1.41905198, 1.10602244, 1.10854133, 1.75360659, 1.04831692,\n",
            "       1.09983971, 1.51454087, 1.07579574, 1.08999313, 1.20059537,\n",
            "       1.09663384, 1.05472865, 1.93771468, 1.06457522, 1.16670483,\n",
            "       1.07785665, 1.14014197, 1.08060453, 1.08106251, 1.20952599,\n",
            "       1.09594687, 1.10327456, 1.04854591, 1.09800779, 1.13373025,\n",
            "       1.11426609, 1.09846577, 1.50583925, 1.12594458, 1.0838104 ,\n",
            "       1.17151362, 1.08655828, 1.03984429, 1.09022212, 1.05953744,\n",
            "       1.36523929, 1.13144035, 1.01900618, 1.07487978, 1.22349439,\n",
            "       1.19097779, 1.0767117 , 1.35607969, 1.64048546, 1.16326998,\n",
            "       1.1254866 , 1.1023586 , 1.11174719, 1.15937715, 1.29722922,\n",
            "       1.04396611, 1.14838562, 1.0487749 , 1.08747424, 1.06617815,\n",
            "       1.12067781, 1.12594458, 1.06801008, 1.23471491, 1.51362491,\n",
            "       1.35928555, 1.07694069, 2.        , 1.44126403, 1.0861003 ,\n",
            "       1.13395924, 1.07762766, 1.11083123, 1.03274559, 1.06365926,\n",
            "       1.10327456, 1.45019464, 1.04762995, 1.05862148, 1.07373483,\n",
            "       1.08701626, 1.0838104 , 1.0396153 , 1.06663613, 1.01557133]),)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "80 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 752, in fit\n",
            "    return self._fit(X, y, incremental=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 393, in _fit\n",
            "    X, y = self._validate_input(X, y, incremental, reset=first_pass)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 1131, in _validate_input\n",
            "    self._label_binarizer.fit(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py\", line 301, in fit\n",
            "    self.classes_ = unique_labels(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py\", line 101, in unique_labels\n",
            "    raise ValueError(\"Unknown label type: %s\" % repr(ys))\n",
            "ValueError: Unknown label type: (array([1.05198076, 1.10877032, 1.09342798, 1.08106251, 1.2441035 ,\n",
            "       1.11609801, 1.02518892, 1.13304328, 1.06686512, 1.04076025,\n",
            "       1.07465079, 1.20036638, 1.22647126, 1.04350813, 1.0604534 ,\n",
            "       1.08770323, 1.09319899, 1.12525761, 1.12296771, 1.15754523,\n",
            "       1.06869705, 1.01144951, 1.09457293, 1.4000458 , 1.10968628,\n",
            "       1.12686054, 1.20563316, 1.09983971, 1.30799176, 1.2466224 ,\n",
            "       1.03801237, 1.02999771, 1.06411724, 1.05885047, 1.0883902 ,\n",
            "       1.14220289, 1.17128463, 1.10579345, 1.09251202, 1.05930845,\n",
            "       1.10373254, 1.11380811, 1.02953973, 1.51957866, 1.04373712,\n",
            "       1.25166018, 1.21685368, 1.05541562, 1.10029769, 1.16326998,\n",
            "       1.00526677, 1.09457293, 1.00549576, 1.07716968, 1.08701626,\n",
            "       1.05793451, 1.08106251, 1.05449966, 1.07190291, 1.14953057,\n",
            "       1.40531257, 1.45088161, 1.12846348, 1.11266316, 1.30272498,\n",
            "       1.06778109, 1.13464621, 1.14014197, 1.09022212, 1.04030227,\n",
            "       1.75727044, 1.1348752 , 1.22120449, 1.08197847, 1.10029769,\n",
            "       1.0977788 , 1.03595145, 1.08976414, 1.1209068 , 1.09800779,\n",
            "       1.02610488, 1.28028395, 1.0790016 , 1.29013052, 1.37829173,\n",
            "       1.06365926, 1.06388825, 1.04167621, 1.15846119, 1.10121365,\n",
            "       1.05289673, 1.17242959, 1.07167392, 1.08541333, 1.06869705,\n",
            "       1.31302954, 1.29379437, 1.03755439, 1.07304786, 1.07098695,\n",
            "       1.08884818, 1.09136707, 1.0487749 , 1.0279368 , 1.03366155,\n",
            "       1.13144035, 1.10396153, 1.07098695, 1.19601557, 1.0510648 ,\n",
            "       1.07579574, 1.30776277, 1.35104191, 1.05724754, 1.03663842,\n",
            "       1.07968857, 1.03595145, 1.19853446, 1.08541333, 1.03801237,\n",
            "       1.03366155, 1.09892375, 1.08037554, 1.00183192, 1.05060682,\n",
            "       1.86764369, 1.09754981, 1.05678956, 1.28990153, 1.10716739,\n",
            "       1.09686283, 1.05999542, 1.05472865, 1.15800321, 1.08197847,\n",
            "       1.10670941, 1.31669338, 1.07281887, 1.08564232, 1.05472865,\n",
            "       1.09663384, 1.15937715, 1.04350813, 1.53423403, 1.10579345,\n",
            "       1.07602473, 1.00137394, 1.1417449 , 1.07625372, 1.0767117 ,\n",
            "       1.06205633, 1.11060224, 1.11518205, 1.03640943, 1.10441951,\n",
            "       1.07739867, 1.08861919, 1.04488207, 1.05907946, 1.35699565,\n",
            "       1.09480192, 1.09045111, 1.03572246, 1.07029998, 1.27845203,\n",
            "       1.08793222, 1.25051523, 1.14815663, 1.06778109, 1.12846348,\n",
            "       1.11930387, 1.07923059, 1.22578429, 1.12892146, 1.2905885 ,\n",
            "       1.07281887, 1.13144035, 1.46072819, 1.07419281, 1.1046485 ,\n",
            "       1.06182734, 1.48477215, 1.09274101, 1.14357683, 1.13166934,\n",
            "       1.16487291, 1.26929242, 1.0767117 , 1.03320357, 1.23150905,\n",
            "       1.05266774, 1.08770323, 1.09869476, 1.08266545, 1.07831463,\n",
            "       1.04831692, 1.03915732, 1.09136707, 1.04236318, 1.14678269,\n",
            "       1.09594687, 1.06686512, 1.08037554, 1.31531944, 1.53858484,\n",
            "       1.15136249, 1.25005725, 1.07831463, 1.20861003, 1.069842  ,\n",
            "       1.12960843, 1.12686054, 1.08312343, 1.01923517, 1.05129379,\n",
            "       1.03824136, 1.14586673, 1.02931074, 1.04007328, 1.09869476,\n",
            "       1.15456835, 1.04282116, 1.00137394, 1.17838333, 1.08907717,\n",
            "       1.0977788 , 1.08152049, 1.08060453, 1.05564461, 1.26402565,\n",
            "       1.02404397, 1.0373254 , 1.08403939, 1.12983742, 1.07281887,\n",
            "       1.02106709, 1.05885047, 1.11174719, 1.11357912, 1.41905198,\n",
            "       1.10602244, 1.10854133, 1.75360659, 1.07579574, 1.98671857,\n",
            "       1.04831692, 1.1813602 , 1.51454087, 1.07579574, 1.20059537,\n",
            "       1.09663384, 1.05472865, 1.93771468, 1.09113808, 1.16670483,\n",
            "       1.05930845, 1.13624914, 1.14014197, 1.09022212, 1.08060453,\n",
            "       1.08106251, 1.20952599, 1.10327456, 1.04854591, 1.12502862,\n",
            "       1.10945729, 1.08587131, 1.09800779, 1.13373025, 1.09846577,\n",
            "       1.50583925, 1.0838104 , 1.17151362, 1.08655828, 1.03984429,\n",
            "       1.09022212, 1.36523929, 1.08426838, 1.19097779, 1.08472636,\n",
            "       1.0767117 , 1.35607969, 1.64048546, 1.16326998, 1.1254866 ,\n",
            "       1.1023586 , 1.01534234, 1.11174719, 1.08655828, 1.15937715,\n",
            "       1.06457522, 1.02862377, 1.04396611, 1.08747424, 1.06617815,\n",
            "       1.0861003 , 1.12067781, 1.0022899 , 1.12594458, 1.06801008,\n",
            "       1.23471491, 1.51362491, 1.35928555, 2.        , 1.0861003 ,\n",
            "       1.13395924, 1.07762766, 1.12823449, 1.11083123, 1.06365926,\n",
            "       1.10327456, 1.10831234, 1.1000687 , 1.45019464, 1.07373483,\n",
            "       1.08701626, 1.0838104 , 1.0396153 , 1.06663613, 1.01557133]),)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "80 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 752, in fit\n",
            "    return self._fit(X, y, incremental=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 393, in _fit\n",
            "    X, y = self._validate_input(X, y, incremental, reset=first_pass)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 1131, in _validate_input\n",
            "    self._label_binarizer.fit(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py\", line 301, in fit\n",
            "    self.classes_ = unique_labels(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py\", line 101, in unique_labels\n",
            "    raise ValueError(\"Unknown label type: %s\" % repr(ys))\n",
            "ValueError: Unknown label type: (array([1.05198076, 1.10877032, 1.08106251, 1.2441035 , 1.06801008,\n",
            "       1.02518892, 1.13304328, 1.07098695, 1.06411724, 1.05381269,\n",
            "       1.07762766, 1.04076025, 1.01923517, 1.07465079, 1.22647126,\n",
            "       1.04350813, 1.0604534 , 1.04236318, 1.08770323, 1.09319899,\n",
            "       1.05381269, 1.12296771, 1.03068468, 1.15754523, 1.08793222,\n",
            "       1.01144951, 1.09457293, 1.4000458 , 1.10968628, 1.12686054,\n",
            "       1.20563316, 1.18433707, 1.09983971, 1.30799176, 1.11564003,\n",
            "       1.06411724, 1.10487749, 1.05885047, 1.0883902 , 1.2603618 ,\n",
            "       1.14220289, 1.16006412, 1.09594687, 1.10373254, 1.11380811,\n",
            "       1.02953973, 1.51957866, 1.04373712, 1.25166018, 1.1673918 ,\n",
            "       1.21685368, 1.05541562, 1.18021525, 1.10029769, 1.16326998,\n",
            "       1.09457293, 1.00549576, 1.05793451, 1.08106251, 1.05449966,\n",
            "       1.07190291, 1.14953057, 1.40531257, 1.12846348, 1.11266316,\n",
            "       1.13464621, 1.09022212, 1.11357912, 1.04030227, 1.75727044,\n",
            "       1.09411495, 1.08197847, 1.10029769, 1.03595145, 1.08976414,\n",
            "       1.09800779, 1.10052668, 1.02610488, 1.07007099, 1.28028395,\n",
            "       1.0790016 , 1.29013052, 1.37829173, 1.06365926, 1.04167621,\n",
            "       1.15846119, 1.10121365, 1.05289673, 1.07167392, 1.07739867,\n",
            "       1.78337531, 1.08541333, 1.06869705, 1.31302954, 1.29379437,\n",
            "       1.03755439, 1.07304786, 1.07098695, 1.08884818, 1.09136707,\n",
            "       1.0487749 , 1.10350355, 1.0279368 , 1.03366155, 1.13144035,\n",
            "       1.10396153, 1.03938631, 1.19601557, 1.47858942, 1.06091138,\n",
            "       1.0510648 , 1.07579574, 1.30776277, 1.35104191, 1.05060682,\n",
            "       1.03663842, 1.07968857, 1.03595145, 1.19853446, 1.08541333,\n",
            "       1.03801237, 1.03366155, 1.16372796, 1.09205404, 1.30844974,\n",
            "       1.09892375, 1.08037554, 1.00183192, 1.05060682, 1.86764369,\n",
            "       1.09754981, 1.28990153, 1.0208381 , 1.10716739, 1.09686283,\n",
            "       1.05999542, 1.05472865, 1.15800321, 1.08197847, 1.10670941,\n",
            "       1.31669338, 1.07281887, 1.08564232, 1.01122052, 1.09663384,\n",
            "       1.15937715, 1.04350813, 1.53423403, 1.10579345, 1.07602473,\n",
            "       1.00137394, 1.07625372, 1.0767117 , 1.11060224, 1.03640943,\n",
            "       1.0370964 , 1.02060911, 1.36730021, 1.08861919, 1.09045111,\n",
            "       1.04488207, 1.05907946, 1.35699565, 1.09045111, 1.03572246,\n",
            "       1.07029998, 1.27845203, 1.08793222, 1.25051523, 1.14815663,\n",
            "       1.06778109, 1.1069384 , 1.12846348, 1.03389054, 1.09892375,\n",
            "       1.11930387, 1.07923059, 1.22578429, 1.18731395, 1.12892146,\n",
            "       1.07281887, 1.07029998, 1.46072819, 1.07419281, 1.2349439 ,\n",
            "       1.1046485 , 1.11678498, 1.0581635 , 1.27272727, 1.06182734,\n",
            "       1.48477215, 1.14357683, 1.16487291, 1.26929242, 1.0767117 ,\n",
            "       1.03320357, 1.01557133, 1.05266774, 1.08770323, 1.09869476,\n",
            "       1.17197161, 1.07831463, 1.03915732, 1.09136707, 1.14678269,\n",
            "       1.11128921, 1.06686512, 1.08037554, 1.31531944, 1.53858484,\n",
            "       1.15136249, 1.25005725, 1.07831463, 1.04465308, 1.09823678,\n",
            "       1.14586673, 1.20861003, 1.069842  , 1.12686054, 1.01923517,\n",
            "       1.15273643, 1.68811541, 1.14586673, 1.04007328, 1.09869476,\n",
            "       1.15456835, 1.04282116, 1.0558736 , 1.08907717, 1.08770323,\n",
            "       1.0977788 , 1.08152049, 1.08060453, 1.26402565, 1.08403939,\n",
            "       1.10533547, 1.12983742, 1.02106709, 1.15960614, 1.16784978,\n",
            "       1.11174719, 1.11357912, 1.41905198, 1.10602244, 1.07579574,\n",
            "       1.98671857, 1.04831692, 1.09983971, 1.1813602 , 1.08999313,\n",
            "       1.20059537, 1.09663384, 1.09113808, 1.06457522, 1.16670483,\n",
            "       1.07785665, 1.05930845, 1.13624914, 1.14014197, 1.09022212,\n",
            "       1.08106251, 1.20952599, 1.09594687, 1.10327456, 1.12502862,\n",
            "       1.10945729, 1.08587131, 1.13373025, 1.11426609, 1.09846577,\n",
            "       1.50583925, 1.12594458, 1.0838104 , 1.03984429, 1.05953744,\n",
            "       1.36523929, 1.13144035, 1.01900618, 1.08426838, 1.07487978,\n",
            "       1.22349439, 1.19097779, 1.08472636, 1.0767117 , 1.35607969,\n",
            "       1.64048546, 1.16326998, 1.1254866 , 1.01534234, 1.08655828,\n",
            "       1.15937715, 1.06457522, 1.02862377, 1.29722922, 1.04396611,\n",
            "       1.14838562, 1.0487749 , 1.08747424, 1.06617815, 1.0861003 ,\n",
            "       1.0022899 , 1.12594458, 1.23471491, 1.07694069, 1.44126403,\n",
            "       1.0861003 , 1.13395924, 1.12823449, 1.11083123, 1.03274559,\n",
            "       1.06365926, 1.10327456, 1.10831234, 1.1000687 , 1.45019464,\n",
            "       1.04762995, 1.05862148, 1.07373483, 1.0838104 , 1.0396153 ]),)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "80 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 752, in fit\n",
            "    return self._fit(X, y, incremental=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 393, in _fit\n",
            "    X, y = self._validate_input(X, y, incremental, reset=first_pass)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 1131, in _validate_input\n",
            "    self._label_binarizer.fit(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py\", line 301, in fit\n",
            "    self.classes_ = unique_labels(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py\", line 101, in unique_labels\n",
            "    raise ValueError(\"Unknown label type: %s\" % repr(ys))\n",
            "ValueError: Unknown label type: (array([1.05198076, 1.09342798, 1.2441035 , 1.11609801, 1.06801008,\n",
            "       1.07098695, 1.06686512, 1.06411724, 1.05381269, 1.07762766,\n",
            "       1.04076025, 1.01923517, 1.07465079, 1.20036638, 1.04350813,\n",
            "       1.0604534 , 1.04236318, 1.09319899, 1.05381269, 1.12525761,\n",
            "       1.12296771, 1.03068468, 1.15754523, 1.08793222, 1.06869705,\n",
            "       1.01144951, 1.4000458 , 1.10968628, 1.12686054, 1.20563316,\n",
            "       1.18433707, 1.09983971, 1.2466224 , 1.03801237, 1.11564003,\n",
            "       1.02999771, 1.06411724, 1.10487749, 1.2603618 , 1.14220289,\n",
            "       1.16006412, 1.17128463, 1.09594687, 1.10579345, 1.09251202,\n",
            "       1.05930845, 1.51957866, 1.1673918 , 1.05541562, 1.18021525,\n",
            "       1.16326998, 1.00526677, 1.09457293, 1.07716968, 1.08701626,\n",
            "       1.05793451, 1.08106251, 1.14953057, 1.40531257, 1.45088161,\n",
            "       1.12846348, 1.30272498, 1.06778109, 1.13464621, 1.14014197,\n",
            "       1.09022212, 1.11357912, 1.04030227, 1.75727044, 1.1348752 ,\n",
            "       1.09411495, 1.22120449, 1.0977788 , 1.03595145, 1.08976414,\n",
            "       1.1209068 , 1.09800779, 1.10052668, 1.07007099, 1.28028395,\n",
            "       1.0790016 , 1.29013052, 1.37829173, 1.06365926, 1.06388825,\n",
            "       1.04167621, 1.05289673, 1.17242959, 1.07167392, 1.07739867,\n",
            "       1.78337531, 1.08541333, 1.06869705, 1.31302954, 1.03755439,\n",
            "       1.07304786, 1.08884818, 1.0487749 , 1.10350355, 1.0279368 ,\n",
            "       1.03366155, 1.13144035, 1.10396153, 1.03938631, 1.07098695,\n",
            "       1.47858942, 1.06091138, 1.0510648 , 1.07579574, 1.35104191,\n",
            "       1.05724754, 1.05060682, 1.07968857, 1.03595145, 1.08541333,\n",
            "       1.03801237, 1.16372796, 1.09205404, 1.30844974, 1.08037554,\n",
            "       1.00183192, 1.05060682, 1.05678956, 1.28990153, 1.0208381 ,\n",
            "       1.10716739, 1.09686283, 1.15800321, 1.08197847, 1.10670941,\n",
            "       1.08564232, 1.05472865, 1.01122052, 1.15937715, 1.04350813,\n",
            "       1.53423403, 1.07602473, 1.00137394, 1.1417449 , 1.0767117 ,\n",
            "       1.06205633, 1.11060224, 1.11518205, 1.03640943, 1.0370964 ,\n",
            "       1.02060911, 1.10441951, 1.07739867, 1.36730021, 1.09045111,\n",
            "       1.04488207, 1.05907946, 1.35699565, 1.09480192, 1.09045111,\n",
            "       1.07029998, 1.27845203, 1.08793222, 1.25051523, 1.1069384 ,\n",
            "       1.03389054, 1.09892375, 1.07923059, 1.22578429, 1.18731395,\n",
            "       1.12892146, 1.2905885 , 1.07029998, 1.13144035, 1.07419281,\n",
            "       1.2349439 , 1.11678498, 1.0581635 , 1.27272727, 1.06182734,\n",
            "       1.09274101, 1.14357683, 1.13166934, 1.03320357, 1.23150905,\n",
            "       1.01557133, 1.05266774, 1.08770323, 1.09869476, 1.17197161,\n",
            "       1.08266545, 1.07831463, 1.04831692, 1.09136707, 1.04236318,\n",
            "       1.09594687, 1.11128921, 1.06686512, 1.08037554, 1.31531944,\n",
            "       1.53858484, 1.15136249, 1.25005725, 1.07831463, 1.04465308,\n",
            "       1.09823678, 1.14586673, 1.20861003, 1.069842  , 1.12960843,\n",
            "       1.12686054, 1.08312343, 1.01923517, 1.15273643, 1.05129379,\n",
            "       1.03824136, 1.68811541, 1.14586673, 1.02931074, 1.04007328,\n",
            "       1.09869476, 1.04282116, 1.0558736 , 1.00137394, 1.17838333,\n",
            "       1.08770323, 1.08060453, 1.05564461, 1.26402565, 1.02404397,\n",
            "       1.0373254 , 1.08403939, 1.10533547, 1.07281887, 1.02106709,\n",
            "       1.05885047, 1.15960614, 1.16784978, 1.11357912, 1.41905198,\n",
            "       1.10854133, 1.75360659, 1.07579574, 1.98671857, 1.04831692,\n",
            "       1.09983971, 1.1813602 , 1.51454087, 1.07579574, 1.08999313,\n",
            "       1.20059537, 1.09663384, 1.05472865, 1.93771468, 1.09113808,\n",
            "       1.06457522, 1.16670483, 1.07785665, 1.05930845, 1.13624914,\n",
            "       1.14014197, 1.09022212, 1.08060453, 1.20952599, 1.09594687,\n",
            "       1.10327456, 1.04854591, 1.12502862, 1.10945729, 1.08587131,\n",
            "       1.09800779, 1.13373025, 1.11426609, 1.50583925, 1.12594458,\n",
            "       1.0838104 , 1.17151362, 1.08655828, 1.09022212, 1.05953744,\n",
            "       1.13144035, 1.01900618, 1.08426838, 1.07487978, 1.22349439,\n",
            "       1.19097779, 1.08472636, 1.64048546, 1.16326998, 1.1254866 ,\n",
            "       1.1023586 , 1.01534234, 1.11174719, 1.08655828, 1.06457522,\n",
            "       1.02862377, 1.29722922, 1.14838562, 1.0487749 , 1.08747424,\n",
            "       1.0861003 , 1.12067781, 1.0022899 , 1.06801008, 1.23471491,\n",
            "       1.51362491, 1.35928555, 1.07694069, 2.        , 1.44126403,\n",
            "       1.0861003 , 1.13395924, 1.07762766, 1.12823449, 1.11083123,\n",
            "       1.03274559, 1.10831234, 1.1000687 , 1.04762995, 1.05862148,\n",
            "       1.08701626, 1.0838104 , 1.0396153 , 1.06663613, 1.01557133]),)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "80 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 752, in fit\n",
            "    return self._fit(X, y, incremental=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 393, in _fit\n",
            "    X, y = self._validate_input(X, y, incremental, reset=first_pass)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 1131, in _validate_input\n",
            "    self._label_binarizer.fit(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py\", line 301, in fit\n",
            "    self.classes_ = unique_labels(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py\", line 101, in unique_labels\n",
            "    raise ValueError(\"Unknown label type: %s\" % repr(ys))\n",
            "ValueError: Unknown label type: (array([1.08106251, 1.2441035 , 1.11609801, 1.06801008, 1.02518892,\n",
            "       1.13304328, 1.06411724, 1.05381269, 1.07762766, 1.07465079,\n",
            "       1.20036638, 1.22647126, 1.0604534 , 1.04236318, 1.08770323,\n",
            "       1.05381269, 1.12525761, 1.12296771, 1.15754523, 1.08793222,\n",
            "       1.06869705, 1.01144951, 1.09457293, 1.4000458 , 1.12686054,\n",
            "       1.20563316, 1.09983971, 1.30799176, 1.2466224 , 1.03801237,\n",
            "       1.02999771, 1.10487749, 1.05885047, 1.2603618 , 1.14220289,\n",
            "       1.16006412, 1.17128463, 1.09594687, 1.10579345, 1.09251202,\n",
            "       1.05930845, 1.10373254, 1.11380811, 1.02953973, 1.51957866,\n",
            "       1.04373712, 1.25166018, 1.1673918 , 1.05541562, 1.18021525,\n",
            "       1.10029769, 1.00526677, 1.09457293, 1.07716968, 1.08701626,\n",
            "       1.05793451, 1.08106251, 1.05449966, 1.07190291, 1.14953057,\n",
            "       1.40531257, 1.12846348, 1.11266316, 1.30272498, 1.06778109,\n",
            "       1.13464621, 1.14014197, 1.09022212, 1.11357912, 1.75727044,\n",
            "       1.09411495, 1.22120449, 1.08197847, 1.10029769, 1.0977788 ,\n",
            "       1.03595145, 1.08976414, 1.1209068 , 1.07007099, 1.28028395,\n",
            "       1.0790016 , 1.29013052, 1.37829173, 1.06365926, 1.06388825,\n",
            "       1.04167621, 1.10121365, 1.05289673, 1.17242959, 1.07167392,\n",
            "       1.07739867, 1.78337531, 1.08541333, 1.06869705, 1.29379437,\n",
            "       1.03755439, 1.07304786, 1.07098695, 1.08884818, 1.09136707,\n",
            "       1.0487749 , 1.10350355, 1.0279368 , 1.03366155, 1.13144035,\n",
            "       1.10396153, 1.03938631, 1.07098695, 1.47858942, 1.06091138,\n",
            "       1.0510648 , 1.07579574, 1.30776277, 1.35104191, 1.05724754,\n",
            "       1.05060682, 1.03663842, 1.07968857, 1.03595145, 1.19853446,\n",
            "       1.08541333, 1.03366155, 1.09205404, 1.30844974, 1.09892375,\n",
            "       1.08037554, 1.00183192, 1.05060682, 1.86764369, 1.05678956,\n",
            "       1.28990153, 1.0208381 , 1.10716739, 1.09686283, 1.05999542,\n",
            "       1.05472865, 1.15800321, 1.08197847, 1.31669338, 1.07281887,\n",
            "       1.08564232, 1.09663384, 1.15937715, 1.04350813, 1.10579345,\n",
            "       1.07602473, 1.00137394, 1.1417449 , 1.07625372, 1.0767117 ,\n",
            "       1.11060224, 1.11518205, 1.03640943, 1.0370964 , 1.10441951,\n",
            "       1.07739867, 1.36730021, 1.08861919, 1.09045111, 1.05907946,\n",
            "       1.35699565, 1.09045111, 1.03572246, 1.07029998, 1.27845203,\n",
            "       1.08793222, 1.14815663, 1.12846348, 1.03389054, 1.09892375,\n",
            "       1.07923059, 1.22578429, 1.18731395, 1.12892146, 1.2905885 ,\n",
            "       1.07281887, 1.13144035, 1.07419281, 1.2349439 , 1.1046485 ,\n",
            "       1.11678498, 1.0581635 , 1.27272727, 1.48477215, 1.09274101,\n",
            "       1.13166934, 1.16487291, 1.26929242, 1.03320357, 1.01557133,\n",
            "       1.05266774, 1.08770323, 1.09869476, 1.17197161, 1.08266545,\n",
            "       1.07831463, 1.04831692, 1.03915732, 1.09136707, 1.04236318,\n",
            "       1.14678269, 1.09594687, 1.11128921, 1.06686512, 1.53858484,\n",
            "       1.15136249, 1.25005725, 1.04465308, 1.09823678, 1.12960843,\n",
            "       1.12686054, 1.01923517, 1.15273643, 1.05129379, 1.03824136,\n",
            "       1.02931074, 1.04007328, 1.09869476, 1.04282116, 1.00137394,\n",
            "       1.17838333, 1.08907717, 1.08770323, 1.08152049, 1.05564461,\n",
            "       1.26402565, 1.02404397, 1.0373254 , 1.08403939, 1.10533547,\n",
            "       1.12983742, 1.05885047, 1.15960614, 1.16784978, 1.11174719,\n",
            "       1.11357912, 1.41905198, 1.10602244, 1.10854133, 1.75360659,\n",
            "       1.07579574, 1.98671857, 1.04831692, 1.09983971, 1.51454087,\n",
            "       1.07579574, 1.08999313, 1.20059537, 1.05472865, 1.93771468,\n",
            "       1.09113808, 1.06457522, 1.07785665, 1.05930845, 1.13624914,\n",
            "       1.14014197, 1.09022212, 1.08060453, 1.08106251, 1.04854591,\n",
            "       1.12502862, 1.08587131, 1.09800779, 1.11426609, 1.09846577,\n",
            "       1.50583925, 1.12594458, 1.0838104 , 1.17151362, 1.03984429,\n",
            "       1.09022212, 1.36523929, 1.13144035, 1.01900618, 1.08426838,\n",
            "       1.07487978, 1.22349439, 1.19097779, 1.08472636, 1.0767117 ,\n",
            "       1.1254866 , 1.01534234, 1.11174719, 1.08655828, 1.15937715,\n",
            "       1.06457522, 1.29722922, 1.04396611, 1.14838562, 1.08747424,\n",
            "       1.06617815, 1.0861003 , 1.12067781, 1.0022899 , 1.12594458,\n",
            "       1.06801008, 1.23471491, 1.51362491, 1.35928555, 1.07694069,\n",
            "       2.        , 1.44126403, 1.0861003 , 1.13395924, 1.07762766,\n",
            "       1.12823449, 1.11083123, 1.03274559, 1.06365926, 1.10327456,\n",
            "       1.10831234, 1.1000687 , 1.45019464, 1.04762995, 1.05862148,\n",
            "       1.07373483, 1.08701626, 1.0396153 , 1.06663613, 1.01557133]),)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "80 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 752, in fit\n",
            "    return self._fit(X, y, incremental=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 393, in _fit\n",
            "    X, y = self._validate_input(X, y, incremental, reset=first_pass)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 1131, in _validate_input\n",
            "    self._label_binarizer.fit(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py\", line 301, in fit\n",
            "    self.classes_ = unique_labels(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py\", line 101, in unique_labels\n",
            "    raise ValueError(\"Unknown label type: %s\" % repr(ys))\n",
            "ValueError: Unknown label type: (array([1.05198076, 1.10877032, 1.09342798, 1.08106251, 1.2441035 ,\n",
            "       1.11609801, 1.06801008, 1.13304328, 1.07098695, 1.06686512,\n",
            "       1.06411724, 1.05381269, 1.04076025, 1.01923517, 1.20036638,\n",
            "       1.22647126, 1.04350813, 1.08770323, 1.09319899, 1.05381269,\n",
            "       1.12525761, 1.12296771, 1.03068468, 1.15754523, 1.08793222,\n",
            "       1.06869705, 1.01144951, 1.09457293, 1.4000458 , 1.10968628,\n",
            "       1.12686054, 1.20563316, 1.18433707, 1.09983971, 1.30799176,\n",
            "       1.2466224 , 1.11564003, 1.02999771, 1.06411724, 1.10487749,\n",
            "       1.05885047, 1.0883902 , 1.2603618 , 1.16006412, 1.09594687,\n",
            "       1.10579345, 1.09251202, 1.10373254, 1.11380811, 1.51957866,\n",
            "       1.25166018, 1.1673918 , 1.21685368, 1.05541562, 1.18021525,\n",
            "       1.16326998, 1.00526677, 1.09457293, 1.00549576, 1.07716968,\n",
            "       1.08701626, 1.05793451, 1.08106251, 1.05449966, 1.14953057,\n",
            "       1.40531257, 1.45088161, 1.12846348, 1.11266316, 1.30272498,\n",
            "       1.06778109, 1.13464621, 1.14014197, 1.11357912, 1.04030227,\n",
            "       1.75727044, 1.1348752 , 1.09411495, 1.10029769, 1.0977788 ,\n",
            "       1.08976414, 1.1209068 , 1.09800779, 1.10052668, 1.02610488,\n",
            "       1.28028395, 1.0790016 , 1.29013052, 1.06388825, 1.15846119,\n",
            "       1.05289673, 1.17242959, 1.07167392, 1.07739867, 1.78337531,\n",
            "       1.08541333, 1.06869705, 1.31302954, 1.03755439, 1.07304786,\n",
            "       1.07098695, 1.08884818, 1.09136707, 1.0279368 , 1.03366155,\n",
            "       1.13144035, 1.10396153, 1.03938631, 1.19601557, 1.06091138,\n",
            "       1.05724754, 1.05060682, 1.03663842, 1.07968857, 1.03595145,\n",
            "       1.19853446, 1.03801237, 1.16372796, 1.09205404, 1.09892375,\n",
            "       1.08037554, 1.05060682, 1.86764369, 1.09754981, 1.0208381 ,\n",
            "       1.10716739, 1.09686283, 1.05999542, 1.15800321, 1.08197847,\n",
            "       1.10670941, 1.07281887, 1.08564232, 1.05472865, 1.01122052,\n",
            "       1.15937715, 1.04350813, 1.53423403, 1.10579345, 1.07602473,\n",
            "       1.00137394, 1.1417449 , 1.07625372, 1.0767117 , 1.06205633,\n",
            "       1.03640943, 1.0370964 , 1.02060911, 1.10441951, 1.07739867,\n",
            "       1.36730021, 1.08861919, 1.09045111, 1.04488207, 1.05907946,\n",
            "       1.35699565, 1.09480192, 1.09045111, 1.03572246, 1.27845203,\n",
            "       1.08793222, 1.25051523, 1.14815663, 1.06778109, 1.1069384 ,\n",
            "       1.12846348, 1.03389054, 1.09892375, 1.11930387, 1.07923059,\n",
            "       1.22578429, 1.18731395, 1.12892146, 1.2905885 , 1.07281887,\n",
            "       1.07029998, 1.13144035, 1.46072819, 1.07419281, 1.2349439 ,\n",
            "       1.0581635 , 1.06182734, 1.48477215, 1.09274101, 1.14357683,\n",
            "       1.0767117 , 1.03320357, 1.23150905, 1.01557133, 1.05266774,\n",
            "       1.08770323, 1.17197161, 1.08266545, 1.04831692, 1.03915732,\n",
            "       1.14678269, 1.09594687, 1.08037554, 1.31531944, 1.53858484,\n",
            "       1.25005725, 1.07831463, 1.04465308, 1.09823678, 1.14586673,\n",
            "       1.20861003, 1.069842  , 1.12960843, 1.12686054, 1.08312343,\n",
            "       1.01923517, 1.15273643, 1.05129379, 1.03824136, 1.68811541,\n",
            "       1.14586673, 1.02931074, 1.09869476, 1.15456835, 1.0558736 ,\n",
            "       1.00137394, 1.08907717, 1.0977788 , 1.08152049, 1.08060453,\n",
            "       1.26402565, 1.02404397, 1.0373254 , 1.10533547, 1.12983742,\n",
            "       1.07281887, 1.02106709, 1.05885047, 1.15960614, 1.16784978,\n",
            "       1.11357912, 1.41905198, 1.10602244, 1.75360659, 1.07579574,\n",
            "       1.04831692, 1.09983971, 1.1813602 , 1.51454087, 1.07579574,\n",
            "       1.08999313, 1.20059537, 1.09663384, 1.05472865, 1.93771468,\n",
            "       1.09113808, 1.06457522, 1.16670483, 1.07785665, 1.05930845,\n",
            "       1.13624914, 1.14014197, 1.09022212, 1.20952599, 1.09594687,\n",
            "       1.10327456, 1.12502862, 1.10945729, 1.08587131, 1.09800779,\n",
            "       1.13373025, 1.11426609, 1.09846577, 1.50583925, 1.12594458,\n",
            "       1.0838104 , 1.08655828, 1.03984429, 1.09022212, 1.05953744,\n",
            "       1.36523929, 1.01900618, 1.08426838, 1.07487978, 1.22349439,\n",
            "       1.19097779, 1.0767117 , 1.35607969, 1.64048546, 1.16326998,\n",
            "       1.1254866 , 1.1023586 , 1.01534234, 1.11174719, 1.08655828,\n",
            "       1.15937715, 1.02862377, 1.04396611, 1.14838562, 1.0487749 ,\n",
            "       1.08747424, 1.06617815, 1.12067781, 1.0022899 , 1.12594458,\n",
            "       1.06801008, 1.23471491, 1.51362491, 1.35928555, 1.07694069,\n",
            "       2.        , 1.44126403, 1.0861003 , 1.12823449, 1.11083123,\n",
            "       1.03274559, 1.06365926, 1.10327456, 1.45019464, 1.04762995,\n",
            "       1.08701626, 1.0838104 , 1.0396153 , 1.06663613, 1.01557133]),)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "80 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 752, in fit\n",
            "    return self._fit(X, y, incremental=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 393, in _fit\n",
            "    X, y = self._validate_input(X, y, incremental, reset=first_pass)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 1131, in _validate_input\n",
            "    self._label_binarizer.fit(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py\", line 301, in fit\n",
            "    self.classes_ = unique_labels(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py\", line 101, in unique_labels\n",
            "    raise ValueError(\"Unknown label type: %s\" % repr(ys))\n",
            "ValueError: Unknown label type: (array([1.05198076, 1.10877032, 1.09342798, 1.08106251, 1.2441035 ,\n",
            "       1.11609801, 1.06801008, 1.02518892, 1.13304328, 1.07098695,\n",
            "       1.06686512, 1.06411724, 1.05381269, 1.07762766, 1.04076025,\n",
            "       1.01923517, 1.07465079, 1.04350813, 1.0604534 , 1.04236318,\n",
            "       1.08770323, 1.09319899, 1.05381269, 1.12525761, 1.12296771,\n",
            "       1.03068468, 1.15754523, 1.08793222, 1.06869705, 1.01144951,\n",
            "       1.09457293, 1.4000458 , 1.10968628, 1.20563316, 1.18433707,\n",
            "       1.09983971, 1.30799176, 1.2466224 , 1.03801237, 1.11564003,\n",
            "       1.06411724, 1.05885047, 1.0883902 , 1.2603618 , 1.14220289,\n",
            "       1.16006412, 1.17128463, 1.09594687, 1.10579345, 1.05930845,\n",
            "       1.10373254, 1.11380811, 1.02953973, 1.04373712, 1.1673918 ,\n",
            "       1.21685368, 1.10029769, 1.16326998, 1.00526677, 1.09457293,\n",
            "       1.00549576, 1.07716968, 1.08701626, 1.05793451, 1.08106251,\n",
            "       1.05449966, 1.07190291, 1.40531257, 1.45088161, 1.30272498,\n",
            "       1.14014197, 1.09022212, 1.11357912, 1.04030227, 1.75727044,\n",
            "       1.1348752 , 1.09411495, 1.22120449, 1.08197847, 1.0977788 ,\n",
            "       1.03595145, 1.1209068 , 1.09800779, 1.10052668, 1.02610488,\n",
            "       1.07007099, 1.37829173, 1.06365926, 1.06388825, 1.04167621,\n",
            "       1.15846119, 1.10121365, 1.05289673, 1.07167392, 1.07739867,\n",
            "       1.06869705, 1.31302954, 1.29379437, 1.03755439, 1.07304786,\n",
            "       1.07098695, 1.08884818, 1.0487749 , 1.10350355, 1.03366155,\n",
            "       1.10396153, 1.03938631, 1.07098695, 1.19601557, 1.47858942,\n",
            "       1.06091138, 1.0510648 , 1.07579574, 1.30776277, 1.35104191,\n",
            "       1.07968857, 1.03595145, 1.19853446, 1.08541333, 1.03801237,\n",
            "       1.03366155, 1.16372796, 1.09205404, 1.30844974, 1.09892375,\n",
            "       1.08037554, 1.00183192, 1.05060682, 1.09754981, 1.05678956,\n",
            "       1.28990153, 1.09686283, 1.05472865, 1.15800321, 1.10670941,\n",
            "       1.31669338, 1.07281887, 1.08564232, 1.05472865, 1.01122052,\n",
            "       1.09663384, 1.04350813, 1.53423403, 1.10579345, 1.1417449 ,\n",
            "       1.07625372, 1.06205633, 1.11060224, 1.11518205, 1.03640943,\n",
            "       1.0370964 , 1.02060911, 1.07739867, 1.09045111, 1.04488207,\n",
            "       1.05907946, 1.35699565, 1.09480192, 1.09045111, 1.03572246,\n",
            "       1.07029998, 1.08793222, 1.25051523, 1.14815663, 1.06778109,\n",
            "       1.1069384 , 1.12846348, 1.03389054, 1.11930387, 1.12892146,\n",
            "       1.07281887, 1.07029998, 1.46072819, 1.2349439 , 1.1046485 ,\n",
            "       1.11678498, 1.0581635 , 1.27272727, 1.06182734, 1.48477215,\n",
            "       1.14357683, 1.13166934, 1.16487291, 1.26929242, 1.0767117 ,\n",
            "       1.23150905, 1.01557133, 1.05266774, 1.08770323, 1.09869476,\n",
            "       1.17197161, 1.08266545, 1.07831463, 1.04831692, 1.09136707,\n",
            "       1.04236318, 1.09594687, 1.11128921, 1.06686512, 1.08037554,\n",
            "       1.31531944, 1.15136249, 1.25005725, 1.07831463, 1.09823678,\n",
            "       1.14586673, 1.20861003, 1.069842  , 1.12960843, 1.12686054,\n",
            "       1.08312343, 1.01923517, 1.15273643, 1.05129379, 1.68811541,\n",
            "       1.14586673, 1.02931074, 1.04007328, 1.15456835, 1.04282116,\n",
            "       1.0558736 , 1.00137394, 1.17838333, 1.08907717, 1.08770323,\n",
            "       1.0977788 , 1.08152049, 1.08060453, 1.05564461, 1.26402565,\n",
            "       1.02404397, 1.0373254 , 1.08403939, 1.12983742, 1.07281887,\n",
            "       1.02106709, 1.16784978, 1.11174719, 1.41905198, 1.10602244,\n",
            "       1.10854133, 1.07579574, 1.98671857, 1.04831692, 1.09983971,\n",
            "       1.1813602 , 1.51454087, 1.07579574, 1.20059537, 1.09663384,\n",
            "       1.05472865, 1.16670483, 1.07785665, 1.05930845, 1.13624914,\n",
            "       1.09022212, 1.08060453, 1.08106251, 1.20952599, 1.09594687,\n",
            "       1.10327456, 1.04854591, 1.12502862, 1.10945729, 1.09800779,\n",
            "       1.13373025, 1.11426609, 1.09846577, 1.50583925, 1.17151362,\n",
            "       1.08655828, 1.03984429, 1.05953744, 1.13144035, 1.01900618,\n",
            "       1.22349439, 1.19097779, 1.08472636, 1.35607969, 1.64048546,\n",
            "       1.16326998, 1.1254866 , 1.1023586 , 1.01534234, 1.11174719,\n",
            "       1.08655828, 1.15937715, 1.06457522, 1.02862377, 1.29722922,\n",
            "       1.04396611, 1.14838562, 1.0487749 , 1.06617815, 1.0861003 ,\n",
            "       1.12067781, 1.12594458, 1.06801008, 1.35928555, 1.07694069,\n",
            "       2.        , 1.44126403, 1.0861003 , 1.13395924, 1.07762766,\n",
            "       1.12823449, 1.11083123, 1.03274559, 1.06365926, 1.10831234,\n",
            "       1.1000687 , 1.45019464, 1.04762995, 1.05862148, 1.07373483,\n",
            "       1.08701626, 1.0838104 , 1.0396153 , 1.06663613, 1.01557133]),)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "80 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 752, in fit\n",
            "    return self._fit(X, y, incremental=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 393, in _fit\n",
            "    X, y = self._validate_input(X, y, incremental, reset=first_pass)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 1131, in _validate_input\n",
            "    self._label_binarizer.fit(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py\", line 301, in fit\n",
            "    self.classes_ = unique_labels(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py\", line 101, in unique_labels\n",
            "    raise ValueError(\"Unknown label type: %s\" % repr(ys))\n",
            "ValueError: Unknown label type: (array([1.05198076, 1.10877032, 1.09342798, 1.08106251, 1.2441035 ,\n",
            "       1.02518892, 1.07098695, 1.06686512, 1.07762766, 1.04076025,\n",
            "       1.01923517, 1.07465079, 1.20036638, 1.22647126, 1.04350813,\n",
            "       1.0604534 , 1.04236318, 1.09319899, 1.12525761, 1.12296771,\n",
            "       1.03068468, 1.08793222, 1.09457293, 1.4000458 , 1.10968628,\n",
            "       1.12686054, 1.20563316, 1.18433707, 1.2466224 , 1.03801237,\n",
            "       1.11564003, 1.02999771, 1.06411724, 1.10487749, 1.0883902 ,\n",
            "       1.14220289, 1.17128463, 1.09594687, 1.09251202, 1.05930845,\n",
            "       1.02953973, 1.51957866, 1.04373712, 1.25166018, 1.21685368,\n",
            "       1.05541562, 1.18021525, 1.10029769, 1.16326998, 1.09457293,\n",
            "       1.00549576, 1.07716968, 1.08701626, 1.05449966, 1.07190291,\n",
            "       1.14953057, 1.45088161, 1.12846348, 1.11266316, 1.06778109,\n",
            "       1.13464621, 1.14014197, 1.09022212, 1.04030227, 1.75727044,\n",
            "       1.1348752 , 1.22120449, 1.08197847, 1.10029769, 1.0977788 ,\n",
            "       1.03595145, 1.08976414, 1.1209068 , 1.09800779, 1.10052668,\n",
            "       1.02610488, 1.07007099, 1.28028395, 1.0790016 , 1.29013052,\n",
            "       1.37829173, 1.06365926, 1.06388825, 1.04167621, 1.15846119,\n",
            "       1.10121365, 1.17242959, 1.07167392, 1.07739867, 1.78337531,\n",
            "       1.08541333, 1.06869705, 1.31302954, 1.29379437, 1.09136707,\n",
            "       1.0487749 , 1.10350355, 1.0279368 , 1.03366155, 1.13144035,\n",
            "       1.03938631, 1.07098695, 1.19601557, 1.47858942, 1.0510648 ,\n",
            "       1.07579574, 1.30776277, 1.35104191, 1.05724754, 1.05060682,\n",
            "       1.03663842, 1.03595145, 1.19853446, 1.08541333, 1.03801237,\n",
            "       1.03366155, 1.16372796, 1.09205404, 1.30844974, 1.08037554,\n",
            "       1.00183192, 1.05060682, 1.86764369, 1.09754981, 1.05678956,\n",
            "       1.28990153, 1.0208381 , 1.10716739, 1.05999542, 1.05472865,\n",
            "       1.08197847, 1.10670941, 1.31669338, 1.07281887, 1.05472865,\n",
            "       1.01122052, 1.09663384, 1.15937715, 1.53423403, 1.07602473,\n",
            "       1.00137394, 1.07625372, 1.0767117 , 1.06205633, 1.11060224,\n",
            "       1.11518205, 1.03640943, 1.02060911, 1.10441951, 1.36730021,\n",
            "       1.08861919, 1.09045111, 1.04488207, 1.05907946, 1.35699565,\n",
            "       1.09480192, 1.03572246, 1.07029998, 1.27845203, 1.25051523,\n",
            "       1.06778109, 1.1069384 , 1.09892375, 1.11930387, 1.07923059,\n",
            "       1.22578429, 1.18731395, 1.2905885 , 1.07029998, 1.13144035,\n",
            "       1.46072819, 1.07419281, 1.1046485 , 1.11678498, 1.27272727,\n",
            "       1.06182734, 1.48477215, 1.09274101, 1.14357683, 1.13166934,\n",
            "       1.16487291, 1.26929242, 1.0767117 , 1.03320357, 1.23150905,\n",
            "       1.05266774, 1.09869476, 1.08266545, 1.07831463, 1.04831692,\n",
            "       1.03915732, 1.09136707, 1.04236318, 1.14678269, 1.09594687,\n",
            "       1.11128921, 1.06686512, 1.08037554, 1.31531944, 1.53858484,\n",
            "       1.15136249, 1.07831463, 1.04465308, 1.09823678, 1.14586673,\n",
            "       1.20861003, 1.069842  , 1.12960843, 1.12686054, 1.08312343,\n",
            "       1.01923517, 1.03824136, 1.68811541, 1.14586673, 1.02931074,\n",
            "       1.04007328, 1.09869476, 1.15456835, 1.04282116, 1.0558736 ,\n",
            "       1.00137394, 1.17838333, 1.08770323, 1.0977788 , 1.08060453,\n",
            "       1.05564461, 1.26402565, 1.02404397, 1.08403939, 1.10533547,\n",
            "       1.07281887, 1.02106709, 1.05885047, 1.15960614, 1.11174719,\n",
            "       1.11357912, 1.10854133, 1.75360659, 1.07579574, 1.98671857,\n",
            "       1.09983971, 1.1813602 , 1.08999313, 1.20059537, 1.09663384,\n",
            "       1.05472865, 1.93771468, 1.09113808, 1.06457522, 1.16670483,\n",
            "       1.07785665, 1.13624914, 1.14014197, 1.08060453, 1.08106251,\n",
            "       1.20952599, 1.09594687, 1.10327456, 1.04854591, 1.12502862,\n",
            "       1.10945729, 1.08587131, 1.09800779, 1.13373025, 1.11426609,\n",
            "       1.09846577, 1.50583925, 1.12594458, 1.0838104 , 1.17151362,\n",
            "       1.08655828, 1.03984429, 1.09022212, 1.05953744, 1.36523929,\n",
            "       1.13144035, 1.01900618, 1.08426838, 1.07487978, 1.22349439,\n",
            "       1.08472636, 1.0767117 , 1.35607969, 1.64048546, 1.16326998,\n",
            "       1.1254866 , 1.1023586 , 1.01534234, 1.11174719, 1.15937715,\n",
            "       1.06457522, 1.02862377, 1.29722922, 1.14838562, 1.0487749 ,\n",
            "       1.08747424, 1.06617815, 1.0861003 , 1.0022899 , 1.06801008,\n",
            "       1.23471491, 1.51362491, 1.35928555, 1.07694069, 2.        ,\n",
            "       1.44126403, 1.0861003 , 1.13395924, 1.07762766, 1.12823449,\n",
            "       1.03274559, 1.10327456, 1.10831234, 1.1000687 , 1.05862148,\n",
            "       1.07373483, 1.0838104 , 1.0396153 , 1.06663613, 1.01557133]),)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "80 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 752, in fit\n",
            "    return self._fit(X, y, incremental=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 393, in _fit\n",
            "    X, y = self._validate_input(X, y, incremental, reset=first_pass)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py\", line 1131, in _validate_input\n",
            "    self._label_binarizer.fit(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/preprocessing/_label.py\", line 301, in fit\n",
            "    self.classes_ = unique_labels(y)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/multiclass.py\", line 101, in unique_labels\n",
            "    raise ValueError(\"Unknown label type: %s\" % repr(ys))\n",
            "ValueError: Unknown label type: (array([1.05198076, 1.10877032, 1.09342798, 1.11609801, 1.06801008,\n",
            "       1.02518892, 1.13304328, 1.07098695, 1.06686512, 1.06411724,\n",
            "       1.05381269, 1.07762766, 1.04076025, 1.01923517, 1.07465079,\n",
            "       1.20036638, 1.22647126, 1.04350813, 1.0604534 , 1.04236318,\n",
            "       1.08770323, 1.09319899, 1.05381269, 1.03068468, 1.15754523,\n",
            "       1.06869705, 1.01144951, 1.10968628, 1.12686054, 1.18433707,\n",
            "       1.09983971, 1.30799176, 1.03801237, 1.11564003, 1.02999771,\n",
            "       1.06411724, 1.10487749, 1.05885047, 1.0883902 , 1.2603618 ,\n",
            "       1.14220289, 1.16006412, 1.17128463, 1.10579345, 1.09251202,\n",
            "       1.05930845, 1.10373254, 1.11380811, 1.02953973, 1.51957866,\n",
            "       1.04373712, 1.25166018, 1.1673918 , 1.21685368, 1.05541562,\n",
            "       1.18021525, 1.10029769, 1.16326998, 1.00526677, 1.00549576,\n",
            "       1.05793451, 1.08106251, 1.07190291, 1.14953057, 1.40531257,\n",
            "       1.45088161, 1.12846348, 1.11266316, 1.30272498, 1.06778109,\n",
            "       1.13464621, 1.09022212, 1.11357912, 1.04030227, 1.1348752 ,\n",
            "       1.09411495, 1.22120449, 1.08197847, 1.10029769, 1.03595145,\n",
            "       1.08976414, 1.09800779, 1.10052668, 1.02610488, 1.07007099,\n",
            "       1.28028395, 1.0790016 , 1.29013052, 1.37829173, 1.06365926,\n",
            "       1.04167621, 1.15846119, 1.10121365, 1.05289673, 1.17242959,\n",
            "       1.78337531, 1.08541333, 1.31302954, 1.29379437, 1.03755439,\n",
            "       1.07304786, 1.07098695, 1.08884818, 1.09136707, 1.0487749 ,\n",
            "       1.10350355, 1.0279368 , 1.13144035, 1.10396153, 1.07098695,\n",
            "       1.19601557, 1.47858942, 1.06091138, 1.0510648 , 1.07579574,\n",
            "       1.30776277, 1.35104191, 1.05724754, 1.05060682, 1.03663842,\n",
            "       1.07968857, 1.08541333, 1.03801237, 1.03366155, 1.16372796,\n",
            "       1.30844974, 1.09892375, 1.00183192, 1.86764369, 1.09754981,\n",
            "       1.05678956, 1.28990153, 1.0208381 , 1.10716739, 1.09686283,\n",
            "       1.05999542, 1.05472865, 1.15800321, 1.08197847, 1.10670941,\n",
            "       1.31669338, 1.08564232, 1.05472865, 1.01122052, 1.09663384,\n",
            "       1.15937715, 1.04350813, 1.53423403, 1.10579345, 1.07602473,\n",
            "       1.00137394, 1.1417449 , 1.0767117 , 1.06205633, 1.11060224,\n",
            "       1.11518205, 1.0370964 , 1.02060911, 1.10441951, 1.07739867,\n",
            "       1.36730021, 1.08861919, 1.04488207, 1.09480192, 1.09045111,\n",
            "       1.07029998, 1.27845203, 1.08793222, 1.25051523, 1.14815663,\n",
            "       1.06778109, 1.1069384 , 1.12846348, 1.03389054, 1.09892375,\n",
            "       1.11930387, 1.07923059, 1.22578429, 1.18731395, 1.12892146,\n",
            "       1.2905885 , 1.07281887, 1.07029998, 1.13144035, 1.46072819,\n",
            "       1.07419281, 1.2349439 , 1.1046485 , 1.11678498, 1.0581635 ,\n",
            "       1.27272727, 1.06182734, 1.09274101, 1.14357683, 1.13166934,\n",
            "       1.16487291, 1.26929242, 1.0767117 , 1.03320357, 1.23150905,\n",
            "       1.01557133, 1.08770323, 1.09869476, 1.17197161, 1.07831463,\n",
            "       1.03915732, 1.09136707, 1.04236318, 1.14678269, 1.11128921,\n",
            "       1.06686512, 1.08037554, 1.31531944, 1.53858484, 1.15136249,\n",
            "       1.25005725, 1.07831463, 1.04465308, 1.14586673, 1.20861003,\n",
            "       1.069842  , 1.08312343, 1.15273643, 1.05129379, 1.03824136,\n",
            "       1.68811541, 1.14586673, 1.04007328, 1.09869476, 1.15456835,\n",
            "       1.04282116, 1.0558736 , 1.17838333, 1.08907717, 1.08770323,\n",
            "       1.0977788 , 1.08152049, 1.08060453, 1.05564461, 1.0373254 ,\n",
            "       1.08403939, 1.10533547, 1.12983742, 1.07281887, 1.02106709,\n",
            "       1.05885047, 1.15960614, 1.16784978, 1.11174719, 1.11357912,\n",
            "       1.41905198, 1.10602244, 1.10854133, 1.75360659, 1.98671857,\n",
            "       1.04831692, 1.1813602 , 1.51454087, 1.07579574, 1.08999313,\n",
            "       1.09663384, 1.93771468, 1.09113808, 1.06457522, 1.16670483,\n",
            "       1.05930845, 1.14014197, 1.09022212, 1.08060453, 1.08106251,\n",
            "       1.20952599, 1.09594687, 1.10327456, 1.04854591, 1.10945729,\n",
            "       1.08587131, 1.13373025, 1.12594458, 1.0838104 , 1.17151362,\n",
            "       1.08655828, 1.09022212, 1.05953744, 1.36523929, 1.13144035,\n",
            "       1.08426838, 1.07487978, 1.19097779, 1.08472636, 1.0767117 ,\n",
            "       1.35607969, 1.64048546, 1.16326998, 1.1023586 , 1.08655828,\n",
            "       1.06457522, 1.02862377, 1.29722922, 1.04396611, 1.0487749 ,\n",
            "       1.08747424, 1.0861003 , 1.12067781, 1.0022899 , 1.12594458,\n",
            "       1.23471491, 1.51362491, 1.13395924, 1.07762766, 1.11083123,\n",
            "       1.06365926, 1.10327456, 1.10831234, 1.1000687 , 1.45019464,\n",
            "       1.04762995, 1.05862148, 1.07373483, 1.08701626, 1.0838104 ]),)\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFittedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-209-d93521d17351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mgrid_model_MLP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelo_MLP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"alpha\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"hidden_layer_sizes\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"learning_rate_init\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmi_MAPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#\"hidden_layer_sizes\":np.arange(1,5),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgrid_model_MLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrainFTf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Utilizando la métrica\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_model_MLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"la mejor puntuación obtenida es:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_model_MLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    873\u001b[0m                 \u001b[0;31m# of out will be done in `_insert_error_scores`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m                     \u001b[0m_insert_error_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_insert_error_scores\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msuccessful_score\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All estimators failed to fit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuccessful_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFittedError\u001b[0m: All estimators failed to fit"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-9.**\n",
        "Con los mejores valores de los hiperparámetros encontrados realiza un análisis de la importancia de los factores. Muestra un diagrama de barras de los resultados e incluye tus conclusiones."
      ],
      "metadata": {
        "id": "mASNrZWs8JTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i,x in enumerate(scaled_Y_train_fit_transform):\n",
        "    scaled_Y_train_fit_transform[i]=x.astype('int')"
      ],
      "metadata": {
        "id": "PidSycymq-PQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(scaled_Y_train_fit_transform)"
      ],
      "metadata": {
        "id": "j1idGi9lv8ls",
        "outputId": "59480d4f-9643-4ff2-86f2-43cfd4af4acb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 2. 0. 0. 0.\n",
            " 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 4. 0. 1. 0. 0. 0. 0. 2. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 4. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0.\n",
            " 4. 0. 0. 0. 4. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
            " 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0.\n",
            " 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 3. 0. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train.shape)"
      ],
      "metadata": {
        "id": "cc_bnd2Jiv95",
        "outputId": "4bcc41e2-b55e-4378-8a46-1a97f32cc718",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(400,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(z)"
      ],
      "metadata": {
        "id": "DWG-5x_Axqxs",
        "outputId": "42a46087-e664-49a7-e483-086c260d44c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2126  168  762  306  167 1834  157  403  230  391  297  347 1292  293\n",
            "  333 1275 1613  814  140  340 2119  508 1200  621  705  732 1578  477\n",
            "  269  630  305  559  316  123  152 3430  436 1513  342  278  435  475\n",
            " 2278  985   59  998 1542  119  437  646  395 1101  340 2256  283  220\n",
            "  393  327  319   32  240  699  413  220  471  379  305    9  301  576\n",
            "  441  275 1779 1276  422  287 1146  240  348  456   93  375  327  392\n",
            "  414  392  557  100  289 1356  348  300 1086  774  563   58 1564  465\n",
            "  335  482  236 3798  884  885  774 1331   92  606  156  399  476  248\n",
            " 2342  475  447  357  327  697  594  106 1052  126  363  211  590  134\n",
            "  462  185 1661  403  514  447  268  319  569  760  291 1233  363  236\n",
            "  740  375 2252  907 3316  131  408  924 3300  347 1353  516 1609  408\n",
            "   15  740  196  642  181 1392  351  584  460  454  323 1292  323  176\n",
            " 1905  101  309  397  604  246  445   15  470  660  788  367  981 4318\n",
            "  630  530  363  537  361  684  123  398  239  493  382  453  244  387\n",
            "  263  621  463  351   93  428  404  277  440  132  461  183  656  432\n",
            "  280  351  143  408 1035  180  459  570  390  361  489 2806  593   77\n",
            " 1185  315 1349  393   33  398  401  287  650 1975  165  497  459  340\n",
            "  471  505  389  266  538  438  537  307  724  796  191 1395  365  298\n",
            " 1354  501  377  360  166  484 1978  175  253  389  977  428  346  379\n",
            "  316  429  355  328  251  306 1075 1604  436  187  488  166  259  598\n",
            "  995  169 1756  154  143  271  248 1225 2099  314 1250  559  385   17\n",
            "  361  348  467  309  920 2602 1307  460 1162  483  237  729  583  431\n",
            "  448  137 2361  469  708  504  801 1008  355  262 1790  403  175  252\n",
            "  439  497  194 4376  375  222  280  288  865  305  341  194  232  506\n",
            " 1020  570  340  583  487 1716  445  316 3014  546  280 4104  876  742\n",
            "  268 1034  555  384  345  662  349  403  446  432  706  289  265  233\n",
            "  497  341  505  563  221  441  636   99  843  200  328  628  701  389\n",
            "  757  722  440  370  472  975  583  199  422  492  322  342  279  420\n",
            "  705 1936  469  704 1016  392  411  403]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(scaled_Y_train_fit_transform)"
      ],
      "metadata": {
        "id": "KtAnLTVJxwwb",
        "outputId": "37579993-07e7-466e-aad6-58d62c19f4ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.48477215 0.03640943 0.17242959 0.06801008 0.03618044 0.41790703\n",
            " 0.03389054 0.09022212 0.05060682 0.08747424 0.06594916 0.07739867\n",
            " 0.29379437 0.0650332  0.07419281 0.28990153 0.36730021 0.18433707\n",
            " 0.02999771 0.07579574 0.48316922 0.11426609 0.27272727 0.14014197\n",
            " 0.15937715 0.16555988 0.35928555 0.10716739 0.05953744 0.14220289\n",
            " 0.06778109 0.12594458 0.07029998 0.02610488 0.03274559 0.78337531\n",
            " 0.0977788  0.34440119 0.07625372 0.06159835 0.09754981 0.10670941\n",
            " 0.51957866 0.22349439 0.01144951 0.22647126 0.35104191 0.02518892\n",
            " 0.09800779 0.14586673 0.0883902  0.25005725 0.07579574 0.51454087\n",
            " 0.0627433  0.04831692 0.08793222 0.07281887 0.07098695 0.00526677\n",
            " 0.05289673 0.15800321 0.09251202 0.04831692 0.10579345 0.08472636\n",
            " 0.06778109 0.         0.06686512 0.12983742 0.09892375 0.06091138\n",
            " 0.40531257 0.29013052 0.09457293 0.06365926 0.2603618  0.05289673\n",
            " 0.07762766 0.1023586  0.01923517 0.0838104  0.07281887 0.08770323\n",
            " 0.09274101 0.08770323 0.1254866  0.0208381  0.06411724 0.30844974\n",
            " 0.07762766 0.06663613 0.2466224  0.17517747 0.12686054 0.01122052\n",
            " 0.35607969 0.10441951 0.07465079 0.10831234 0.05198076 0.86764369\n",
            " 0.20036638 0.20059537 0.17517747 0.30272498 0.01900618 0.13670712\n",
            " 0.03366155 0.08930616 0.1069384  0.05472865 0.53423403 0.10670941\n",
            " 0.10029769 0.07968857 0.07281887 0.15754523 0.13395924 0.02221204\n",
            " 0.23883673 0.02679185 0.08106251 0.04625601 0.13304328 0.02862377\n",
            " 0.10373254 0.04030227 0.37829173 0.09022212 0.11564003 0.10029769\n",
            " 0.05930845 0.07098695 0.12823449 0.17197161 0.06457522 0.28028395\n",
            " 0.08106251 0.05198076 0.1673918  0.0838104  0.51362491 0.20563316\n",
            " 0.75727044 0.0279368  0.09136707 0.20952599 0.75360659 0.07739867\n",
            " 0.30776277 0.11609801 0.36638425 0.09136707 0.00137394 0.1673918\n",
            " 0.04282116 0.14495077 0.03938631 0.31669338 0.07831463 0.13166934\n",
            " 0.10327456 0.10190062 0.07190291 0.29379437 0.07190291 0.03824136\n",
            " 0.43416533 0.02106709 0.06869705 0.08884818 0.13624914 0.05427067\n",
            " 0.09983971 0.00137394 0.10556446 0.14907259 0.17838333 0.08197847\n",
            " 0.22257843 0.98671857 0.14220289 0.11930387 0.08106251 0.1209068\n",
            " 0.08060453 0.15456835 0.02610488 0.08907717 0.05266774 0.11083123\n",
            " 0.08541333 0.10167163 0.05381269 0.08655828 0.0581635  0.14014197\n",
            " 0.10396153 0.07831463 0.01923517 0.09594687 0.09045111 0.06136936\n",
            " 0.09869476 0.02816579 0.10350355 0.03984429 0.14815663 0.09686283\n",
            " 0.06205633 0.07831463 0.03068468 0.09136707 0.2349439  0.03915732\n",
            " 0.10304557 0.12846348 0.08724525 0.08060453 0.10991527 0.64048546\n",
            " 0.13373025 0.01557133 0.26929242 0.07007099 0.30684681 0.08793222\n",
            " 0.00549576 0.08907717 0.08976414 0.06365926 0.14678269 0.45019464\n",
            " 0.03572246 0.11174719 0.10304557 0.07579574 0.10579345 0.11357912\n",
            " 0.08701626 0.05885047 0.12113579 0.09823678 0.1209068  0.06823907\n",
            " 0.16372796 0.18021525 0.04167621 0.31738035 0.08152049 0.06617815\n",
            " 0.30799176 0.11266316 0.08426838 0.08037554 0.03595145 0.10877032\n",
            " 0.45088161 0.03801237 0.0558736  0.08701626 0.22166247 0.09594687\n",
            " 0.07716968 0.08472636 0.07029998 0.09617586 0.07923059 0.07304786\n",
            " 0.05541562 0.06801008 0.2441035  0.36523929 0.0977788  0.04076025\n",
            " 0.10968628 0.03595145 0.05724754 0.1348752  0.22578429 0.03663842\n",
            " 0.4000458  0.03320357 0.03068468 0.05999542 0.05472865 0.27845203\n",
            " 0.47858942 0.069842   0.28417678 0.12594458 0.0861003  0.00183192\n",
            " 0.08060453 0.07762766 0.10487749 0.06869705 0.20861003 0.59377147\n",
            " 0.29722922 0.10327456 0.26402565 0.10854133 0.05220975 0.16487291\n",
            " 0.13144035 0.09663384 0.10052668 0.02931074 0.53858484 0.10533547\n",
            " 0.16006412 0.11335013 0.1813602  0.22876116 0.07923059 0.05793451\n",
            " 0.40783146 0.09022212 0.03801237 0.05564461 0.09846577 0.11174719\n",
            " 0.04236318 1.         0.0838104  0.0487749  0.06205633 0.06388825\n",
            " 0.19601557 0.06778109 0.07602473 0.04236318 0.0510648  0.11380811\n",
            " 0.23150905 0.12846348 0.07579574 0.13144035 0.10945729 0.39088619\n",
            " 0.09983971 0.07029998 0.68811541 0.12296771 0.06205633 0.93771468\n",
            " 0.19853446 0.16784978 0.05930845 0.23471491 0.12502862 0.08587131\n",
            " 0.07694069 0.14953057 0.07785665 0.09022212 0.1000687  0.09686283\n",
            " 0.15960614 0.06411724 0.05862148 0.05129379 0.11174719 0.07602473\n",
            " 0.11357912 0.12686054 0.04854591 0.09892375 0.14357683 0.02060911\n",
            " 0.19097779 0.04373712 0.07304786 0.1417449  0.15846119 0.08701626\n",
            " 0.17128463 0.16326998 0.09869476 0.08266545 0.10602244 0.22120449\n",
            " 0.13144035 0.04350813 0.09457293 0.11060224 0.07167392 0.07625372\n",
            " 0.06182734 0.09411495 0.15937715 0.44126403 0.10533547 0.15914816\n",
            " 0.23059308 0.08770323 0.09205404 0.09022212]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MejorModelo_MLP = MLPClassifier(hidden_layer_sizes=(20,),\n",
        "                            alpha=0.0001,\n",
        "                            learning_rate_init=0.00001)\n",
        "\n",
        "X_train_ajustado = columnasTransformer.fit(X_train)\n",
        "\n",
        "X_train_ajustado_transformado = X_train_ajustado.transform(X_train)\n",
        "\n",
        "scaler=MinMaxScaler()\n",
        "z = np.array(y_train)\n",
        "z.reshape(1, -1)\n",
        "scaled_Y_train_fit = scaler.fit(z.reshape(-1,1))\n",
        "scaled_Y_train_fit_transform = scaler.transform(z.reshape(-1,1))\n",
        "\n",
        "scaled_Y_train_fit_transform = scaled_Y_train_fit_transform.reshape(-1)\n",
        "\n",
        "MejorModelo_MLP.fit(X_train_ajustado_transformado, y_train)\n",
        "\n",
        "Importancia_Factores = permutation_importance(MejorModelo_MLP, X_train_ajustado_transformado, scaled_Y_train_fit_transform, n_repeats=10)\n",
        "\n",
        "print(Importancia_Factores)"
      ],
      "metadata": {
        "id": "X6HJP9hb8LCp",
        "outputId": "b76a4659-ba07-4cac-9150-4f5840bae20d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-4241eb262830>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mMejorModelo_MLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_ajustado_transformado\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mImportancia_Factores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpermutation_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMejorModelo_MLP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_ajustado_transformado\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaled_Y_train_fit_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_repeats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImportancia_Factores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/inspection/_permutation_importance.py\u001b[0m in \u001b[0;36mpermutation_importance\u001b[0;34m(estimator, X, y, scoring, n_repeats, n_jobs, random_state, sample_weight, max_samples)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_MultimetricScorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mscorers_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m     \u001b[0mbaseline_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_weights_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     scores = Parallel(n_jobs=n_jobs)(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/inspection/_permutation_importance.py\u001b[0m in \u001b[0;36m_weights_scorer\u001b[0;34m(scorer, estimator, X, y, sample_weight)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_scorer.py\u001b[0m in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_passthrough_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;34m\"\"\"Function that wraps estimator.score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     93\u001b[0m         raise ValueError(\n\u001b[1;32m     94\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             )\n\u001b[1;32m     97\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous and binary targets"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, j in enumerate(Importancia_Factores['importances_mean']):\n",
        "\tprint('Atributo: %0d, Puntuación: %.5f' % (i,j))\n",
        "\n",
        "plt.bar([x for x in range(len(Importancia_Factores['importances_mean']))], Importancia_Factores['importances_mean'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xr6oe1VF8K_A",
        "outputId": "5da583a6-f7f9-491c-f6bd-b9e7f5e83154",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Atributo: 0, Puntuación: 0.00000\n",
            "Atributo: 1, Puntuación: 0.00000\n",
            "Atributo: 2, Puntuación: 0.00000\n",
            "Atributo: 3, Puntuación: 0.00000\n",
            "Atributo: 4, Puntuación: 0.00000\n",
            "Atributo: 5, Puntuación: 0.00000\n",
            "Atributo: 6, Puntuación: 0.00000\n",
            "Atributo: 7, Puntuación: 0.00000\n",
            "Atributo: 8, Puntuación: 0.00000\n",
            "Atributo: 9, Puntuación: 0.00000\n",
            "Atributo: 10, Puntuación: 0.00000\n",
            "Atributo: 11, Puntuación: 0.00000\n",
            "Atributo: 12, Puntuación: 0.00000\n",
            "Atributo: 13, Puntuación: 0.00000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAAD7CAYAAACPICYfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAT7ElEQVR4nO3df2xVd/3H8VfvJWaMQuDWtty7dpKZjFS3smQkaJSoeNvbhQtlJOXGbkuc2JmAMkw0onGUFn91f0wQ6CaG/WFmlq7RVVo76BpIEBfnNHNCuqnpekfHvaX1FuTHJuq95/sHs991hZVyz+1Zzvv5+O/ufOh9v1dynr0nBIocx3EEAAB8LeD1AAAAoPAIPgAABhB8AAAMIPgAABhA8AEAMIDgAwBgAMEHAMCAOV4P4KazZy8pl/PmrxUoKSlWJnPRk/eebezqT+zqP1b2lGzuGggUadGiedf963wV/FzO8Sz4/3t/K9jVn9jVf6zsKbHrdHikDwCAAQQfAAADCD4AAAYQfAAADCD4AAAYQPABADCA4AMAYADBBwDAAIIPAIABBB8AAAMIPgAABhB8AAAMIPgAABhA8AEAMIDgAwBgAMEHAMAAgg8AgAEEHwAAAwg+AAAGEHwAAAwg+AAAGEDwAQAwgOADAGCAa8EfGhpSIpFQLBZTIpFQMpmcciabzaqlpUXRaFQ1NTXq7Oyccub111/XsmXL1NbW5tZoAACY51rwm5ub1djYqMOHD6uxsVHbt2+fcqa7u1unTp1SX1+fOjo6tGfPHr355psT17PZrJqbmxWNRt0aCwAAyKXgZzIZDQwMKB6PS5Li8bgGBgY0Pj4+6Vxvb68aGhoUCAQUCoUUjUZ16NChiev79+/XZz/7WS1ZssSNsQAAwDtcCX46nVZ5ebmCwaAkKRgMqqysTOl0esq5SCQy8TocDmtkZESS9Nprr+n48eP64he/6MZIAADgXeZ4PYAk/ec//9EjjzyiH/7whxM/NNyIkpJiF6eaudLS+Z6+/2xiV39iV/+xsqfErtNxJfjhcFhnzpxRNptVMBhUNpvV6OiowuHwlHOpVErV1dWS/v8T/9jYmE6dOqWHHnpIknT+/Hk5jqOLFy9q586d1z1HJnNRuZzjxkozVlo6X2NjFzx579nGrv7Erv5jZU/J5q6BQNGMPui6EvySkhJVVVWpp6dH9fX16unpUVVVlUKh0KRzdXV16uzsVG1trc6dO6f+/n794he/UCQS0Ysvvjhxbs+ePXrrrbf0rW99y43xAAAwz7U/pb9jxw499dRTisVieuqpp9TS0iJJampq0okTJyRJ9fX1qqioUG1trTZs2KDNmzersrLSrREAAMA1FDmO480z8ALgkf7sYFd/Ylf/sbKnZHPXmT7S52/aAwDAAIIPAIABBB8AAAMIPgAABhB8AAAMIPgAABhA8AEAMIDgAwBgAMEHAMAAgg8AgAEEHwAAAwg+AAAGEHwAAAwg+AAAGEDwAQAwgOADAGAAwQcAwACCDwCAAQQfAAADCD4AAAYQfAAADCD4AAAYQPABADCA4AMAYADBBwDAAIIPAIABBB8AAAMIPgAABhB8AAAMIPgAABjgWvCHhoaUSCQUi8WUSCSUTCannMlms2ppaVE0GlVNTY06Ozsnru3bt0+rV6/WmjVrtH79ev32t791azQAAMyb49YXam5uVmNjo+rr6/XrX/9a27dv189//vNJZ7q7u3Xq1Cn19fXp3LlzWrdunT75yU+qoqJC1dXV+tKXvqS5c+fqtdde0/3336/jx4/rpptucmtEAADMcuUTfiaT0cDAgOLxuCQpHo9rYGBA4+Pjk8719vaqoaFBgUBAoVBI0WhUhw4dkiStXLlSc+fOlSQtXbpUjuPo3LlzbowHAIB5rgQ/nU6rvLxcwWBQkhQMBlVWVqZ0Oj3lXCQSmXgdDoc1MjIy5et1dXXp1ltv1eLFi90YDwAA81x7pO+WP/zhD9q9e7eefPLJGf/akpLiAkx0/UpL53v6/rOJXf2JXf3Hyp4Su07HleCHw2GdOXNG2WxWwWBQ2WxWo6OjCofDU86lUilVV1dLmvqJ/+WXX9Y3v/lNtbe367bbbpvxHJnMReVyTn7L3KDS0vkaG7vgyXvPNnb1J3b1Hyt7SjZ3DQSKZvRB15VH+iUlJaqqqlJPT48kqaenR1VVVQqFQpPO1dXVqbOzU7lcTuPj4+rv71csFpMk/eUvf9HXv/51/eQnP9HHP/5xN8YCAADvcO2R/o4dO7Rt2za1t7drwYIFamtrkyQ1NTVpy5YtuvPOO1VfX69XXnlFtbW1kqTNmzersrJSktTS0qJ//etf2r59+8TXfPTRR7V06VK3RgQAwKwix3G8eQZeADzSnx3s6k/s6j9W9pRs7urJI30AAPDBRvABADCA4AMAYADBBwDAAIIPAIABBB8AAAMIPgAABhB8AAAMIPgAABhA8AEAMIDgAwBgAMEHAMAAgg8AgAEEHwAAAwg+AAAGEHwAAAwg+AAAGEDwAQAwgOADAGAAwQcAwACCDwCAAQQfAAADCD4AAAYQfAAADCD4AAAYQPABADCA4AMAYADBBwDAAIIPAIABBB8AAAMIPgAABrgW/KGhISUSCcViMSUSCSWTySlnstmsWlpaFI1GVVNTo87Ozuu6BgAA8uNa8Jubm9XY2KjDhw+rsbFR27dvn3Kmu7tbp06dUl9fnzo6OrRnzx69+eab014DAAD5cSX4mUxGAwMDisfjkqR4PK6BgQGNj49POtfb26uGhgYFAgGFQiFFo1EdOnRo2msAACA/rgQ/nU6rvLxcwWBQkhQMBlVWVqZ0Oj3lXCQSmXgdDoc1MjIy7TUAAJCfOV4P4KaSkmJP37+0dL6n7z+b2NWf2NV/rOwpset0XAl+OBzWmTNnlM1mFQwGlc1mNTo6qnA4POVcKpVSdXW1pMmf6t/v2vXKZC4ql3Nc2GjmSkvna2zsgifvPdvY1Z/Y1X+s7CnZ3DUQKJrRB11XHumXlJSoqqpKPT09kqSenh5VVVUpFApNOldXV6fOzk7lcjmNj4+rv79fsVhs2msAACA/rj3S37Fjh7Zt26b29nYtWLBAbW1tkqSmpiZt2bJFd955p+rr6/XKK6+otrZWkrR582ZVVlZK0vteAwAA+SlyHMebZ+AFwCP92cGu/sSu/mNlT8nmrp480gcAAB9sBB8AAAMIPgAABhB8AAAMIPgAABhA8AEAMIDgAwBgAMEHAMAAgg8AgAEEHwAAAwg+AAAGEHwAAAwg+AAAGEDwAQAwgOADAGAAwQcAwACCDwCAAQQfAAADCD4AAAYQfAAADCD4AAAYQPABADCA4AMAYADBBwDAAIIPAIABBB8AAAMIPgAABhB8AAAMIPgAABhA8AEAMCDv4L/99tvaunWrampqVFdXp6NHj17z7DPPPKOamhpFo1G1trYql8tJkvr7+7V+/XrF43GtXr1aTz75ZL5jAQCAd5mT7xc4cOCAiouL9fzzzyuZTOq+++5TX1+f5s2bN+nc8PCw9u7dq66uLi1cuFBNTU06ePCg1q1bp9LSUj3++OMqLy/XhQsXtH79elVXV2v58uX5jgcAAOTCJ/znnntOiURCkrRkyRLdcccdOnbs2JRzhw8fVjQaVSgUUiAQUENDg3p7eyVJy5YtU3l5uSRp/vz5+uhHP6rTp0/nOxoAAHhH3sFPpVK65ZZbJl6Hw2GNjIxMOZdOpxWJRCZeRyIRpdPpKecGBwf15z//WZ/4xCfyHQ0AALxj2kf69957r1Kp1FWvvfDCC64OMzo6qk2bNqm5uXniE/9MlJQUuzrPTJWWzvf0/WcTu/oTu/qPlT0ldp3OtMF/9tln3/d6JBLR6dOnFQqFJF35JL9ixYop58Lh8KQfHFKplMLh8MTrTCajBx98UF/+8pd1zz33XPcC75bJXFQu59zQr81Xael8jY1d8OS9Zxu7+hO7+o+VPSWbuwYCRTP6oJv3I/26ujp1dHRIkpLJpE6cOKGVK1dOOReLxdTf36/x8XHlcjl1dnZOhP3s2bN68MEHdd9996mhoSHfkQAAwHvkHfyNGzfq/Pnzqqmp0Ve+8hW1traquPjKTxy7d+/W008/LUmqrKzUpk2btGHDBtXW1qqiokJr166VJO3fv1/JZFIdHR2qr69XfX29fvnLX+Y7GgAAeEeR4zjePAMvAB7pzw529Sd29R8re0o2d531R/oAAOCDj+ADAGAAwQcAwACCDwCAAQQfAAADCD4AAAYQfAAADCD4AAAYQPABADCA4AMAYADBBwDAAIIPAIABBB8AAAMIPgAABhB8AAAMIPgAABhA8AEAMIDgAwBgAMEHAMAAgg8AgAEEHwAAAwg+AAAGEHwAAAwg+AAAGEDwAQAwgOADAGAAwQcAwACCDwCAAQQfAAADCD4AAAYQfAAADMg7+G+//ba2bt2qmpoa1dXV6ejRo9c8+8wzz6impkbRaFStra3K5XKTrl++fFmrV6/W+vXr8x0LAAC8S97BP3DggIqLi/X888/riSee0He/+11dunRpyrnh4WHt3btXHR0d6uvr0xtvvKGDBw9OOvPjH/9Yy5Yty3ckAADwHnkH/7nnnlMikZAkLVmyRHfccYeOHTs25dzhw4cVjUYVCoUUCATU0NCg3t7eiet//OMflUwmVV9fn+9IAADgPfIOfiqV0i233DLxOhwOa2RkZMq5dDqtSCQy8ToSiSidTkuS3nrrLf3gBz9QS0tLvuMAAICrmDPdgXvvvVepVOqq11544QVXhnj00UfV2Nio8vJyJZPJG/46JSXFrsxzo0pL53v6/rOJXf2JXf3Hyp4Su05n2uA/++yz73s9Eono9OnTCoVCkq58kl+xYsWUc+FweNIPDqlUSuFwWJL0pz/9SceOHVN7e7suX76sf/7zn1qzZo26u7tntEwmc1G5nDOjX+OW0tL5Ghu74Ml7zzZ29Sd29R8re0o2dw0Eimb0QTfvR/p1dXXq6OiQJCWTSZ04cUIrV66cci4Wi6m/v1/j4+PK5XLq7OzUPffcI0nq7u7WkSNHdOTIET322GO6/fbbZxx7AABwbdN+wp/Oxo0btW3bNtXU1CgQCKi1tVXFxVd+4ti9e7fKysr0hS98QZWVldq0aZM2bNggSfrUpz6ltWvX5vv2AADgOhQ5juPNM/AC4JH+7GBXf2JX/7Gyp2Rz11l/pA8AAD74CD4AAAYQfAAADCD4AAAYQPABADCA4AMAYADBBwDAAIIPAIABBB8AAAMIPgAABhB8AAAMIPgAABhA8AEAMIDgAwBgAMEHAMAAgg8AgAEEHwAAAwg+AAAGEHwAAAwg+AAAGEDwAQAwgOADAGAAwQcAwIA5Xg/gpkCgyPT7zyZ29Sd29R8re0r2dp3pvkWO4zgFmgcAAHxA8EgfAAADCD4AAAYQfAAADCD4AAAYQPABADCA4AMAYADBBwDAAIIPAIABBB8AAAMIvguGhoaUSCQUi8WUSCSUTCa9Hsl1Z8+eVVNTk2KxmNasWaOvfvWrGh8f93qsgtu7d6+WLl2qv/3tb16PUjCXL19Wc3OzamtrtWbNGj3yyCNej1QwR48e1bp161RfX6+1a9eqr6/P65Fc09bWplWrVk35/eq3+9PV9vTr/ela39P/mfH9yUHeHnjgAaerq8txHMfp6upyHnjgAY8nct/Zs2ed3//+9xOvf/SjHznf/va3PZyo8E6ePOls3LjR+dznPuf89a9/9Xqcgtm5c6fz/e9/38nlco7jOM7Y2JjHExVGLpdzli9fPvG9fPXVV5277rrLyWazHk/mjpdeeslJpVJTfr/67f50tT39en+61vfUcW7s/sQn/DxlMhkNDAwoHo9LkuLxuAYGBnzx0+W7LVy4UCtWrJh4fddddymVSnk4UWH9+9//Vmtrq3bs2OH1KAV16dIldXV16eGHH1ZR0ZV/iOPDH/6wx1MVTiAQ0IULFyRJFy5cUFlZmQIBf9wGly9frnA4POm/+fH+dLU9/Xp/utqu0o3fn3z1r+V5IZ1Oq7y8XMFgUJIUDAZVVlamdDqtUCjk8XSFkcvl9PTTT2vVqlVej1Iwu3fv1tq1a1VRUeH1KAU1PDyshQsXau/evXrxxRc1b948Pfzww1q+fLnXo7muqKhIu3bt0qZNm3TzzTfr0qVL2r9/v9djFRT3J3+60fuTP360xazauXOnbr75Zt1///1ej1IQL7/8sk6ePKnGxkavRym4bDar4eFhfexjH9OvfvUrfeMb39DXvvY1Xbx40evRXPff//5XP/3pT9Xe3q6jR4/q8ccf19atW3Xp0iWvR4OLuD9dG8HPUzgc1pkzZ5TNZiVduYGOjo5e9TGMH7S1temNN97Qrl27fPMo9L1eeuklDQ4O6vOf/7xWrVqlkZERbdy4UcePH/d6NNeFw2HNmTNn4pHvsmXLtGjRIg0NDXk8mfteffVVjY6O6u6775Yk3X333Zo7d64GBwc9nqxwuD/5Tz73J3/+H5lFJSUlqqqqUk9PjySpp6dHVVVVvnxc9thjj+nkyZPat2+fPvShD3k9TsE89NBDOn78uI4cOaIjR45o8eLFOnDggD796U97PZrrQqGQVqxYod/97neSrvyJ7kwmo4985CMeT+a+xYsXa2RkRK+//rokaXBwUJlMRrfeeqvHkxUO9yf/yef+VOQ4jjMLM/ra4OCgtm3bpvPnz2vBggVqa2vTbbfd5vVYrvr73/+ueDyuJUuW6KabbpIkVVRUaN++fR5PVnirVq3SE088odtvv93rUQpieHhY3/nOd3Tu3DnNmTNHW7du1Wc+8xmvxyqIgwcP6mc/+9nEH1DcsmWLotGox1O543vf+576+vr0j3/8Q4sWLdLChQv1m9/8xnf3p6vtuWvXLl/en671PX23mdyfCD4AAAbwSB8AAAMIPgAABhB8AAAMIPgAABhA8AEAMIDgAwBgAMEHAMAAgg8AgAH/B+6rYDeBL9e9AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-10.**\n",
        "Repite el ejercicio 8 y 9 para el modelo de bosque aleatorio para buscar sus mejores hiperparámetros (realiza la búsqueda con aquellos hiperparámetros que consideres más adecuados) y usando el conjunto de Prueba. Y realiza igualmente el análisis de importancia de factores con este modelo con un diagrama de barras."
      ],
      "metadata": {
        "id": "VUIcDshs8MzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "None"
      ],
      "metadata": {
        "id": "0lKNJNIt8N88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Inkq5YQe8PED"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-11.**\n",
        "Repite el ejercicio 8 y 9 para el modelo de regresión lineal múltiple para buscar sus mejores hiperparámetros (realiza la búsqueda con aquellos hiperparámetros que consideres más adecuados) y usando el conjunto de Prueba. Y realiza igualmente el análisis de importancia de factores con este modelo con un diagrama de barras."
      ],
      "metadata": {
        "id": "5LJl6oql8Pc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "None"
      ],
      "metadata": {
        "id": "-YiSnt9t8RfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2iRA78ZC8Rbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Ejercicio-12.**\n",
        "Compara tus resultados con los obtenidos por los autores del artículo de Moro-Rita-Vala con respecto a MAPE. Incluye tus conclusiones finales de la actividad.\n"
      ],
      "metadata": {
        "id": "IKW72uyk8Sbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "None"
      ],
      "metadata": {
        "id": "gwpz77W38Uq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8v8HL02W8UmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Fin de la Actividad de la semana 7.**"
      ],
      "metadata": {
        "id": "7ql_r2G-DB_m"
      }
    }
  ]
}